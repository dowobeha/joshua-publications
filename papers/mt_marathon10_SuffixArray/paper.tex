%% Sample file $Id: PBML_article.tex 92 2007-12-18 22:59:46Z zw $
% 
% This is a sample file for the PBML article.
% 
% The Prague Bulletin of Mathematical Linguistics is typeset by XeLaTeX. This
% means that it is not guaranteed that the line and page breaks will come up
% exactly the same in the final printed version as in your computer. It is
% not required that you use XeLaTeX and the same fonts as will be used in the
% journal but it is better to do so if you could. You can even used standard
% LaTeX if XeLaTeX is not available on your computer.
% 
% 
% Document class
% ==============
% 
% The Prague Bulletin of Mathematical Linguistics uses its own document class.
% You will thus start your document with:

\documentclass{pbml}


% If you process the document with XeLaTeX, fonts Minion Pro (distributed
% with Adobe Reader) and DejaVu
% (http://dejavu.sourceforge.net/wiki/index.php/Main_Page). If you do not
% have these fonts and are not able to install them, you can instruct XeLaTeX
% to use its default fonts by issuing \document[nofonts]{pbml}.
% 
% 
% Packages
% ========
% 
% I this place you will load required packages by the \usepackage commands.
% Remember that other articles may require other packages and it is known
% that some of them are in conflict. Please use only those packages that you
% really need.
% 
% The following packages are loaded automatically by the class:
% euler (for math fonts)
% graphicx (for inclusion of images)
% multicol (for multicolumn typesetting)
% fullname (for bibliography citations)
% 
% If XeLaTeX is used, fontspec and xltxtra are loaded as well.
% 
% If standard LaTeX is used, the font encoding is switched to T1 and Latin
% Modern fonts are used if they are found in your TeX distribution.
% 
% 
% Graphics
% ========
% 
% The most portable graphics package is tikz which is a front end to pgf.
% This package is fully supported both in standard LaTeX and XeLaTeX.
% PSTricks is not currently available for XeLaTeX. It will most probably be
% supported in the future.
% 
% 
% Fonts
% =====
% 
% If you want to typset examples in east Asian scripts, you have to use
% OpenType Unicode fonts that are freely redistributable and you have to
% include them with your article. If you must use nonfree or non-Unicode
% fonts, you must supply the examples as EPS or PDF with fonts embedded in
% the files (as the required subset).
% 
% 
% Definitions
% ===========
% 
% You can use your own macros defined by \newcommand, \providecommand,
% \DeclareRobustCommand (or even by \def) as well as environments declared by
% \newenvironment. These definitions will automatically be local to your
% article and will not clash with other parts of the journal. You can also
% use \newcounter in order to declare your own counters.
% 
% 
% Beginning of the document
% =========================
% 
% Use the standard command:

%\usepackage{newcite}

%\newcommand{\url}{\begin{verbatim}URL\end{verbatim}}
\usepackage{url}
\usepackage{moreverb}
\usepackage{algorithm}
\usepackage{algpseudocode}


\newcommand{\newcite}{\namecite}


\begin{document}


% Document title
% ==============
% 
% Due to journal organization the article title and authors must be specified
% AFTER \begin{document}. Give the article title:

\title{Hierarchical Phrase-Based Grammar Extraction in Joshua}

% You may optionally specify a subtitle:

\subtitle{Suffix Arrays and Prefix Trees}

% Now put the author names and adresses, one command for each authors. The
% order of authors printed below the title will be the same as the order of
% your commands:

\author{firstname=Lane, surname=Schwartz,
       address={University of Minnesota, Minneapolis}}

\author{firstname=Chris, surname=Callison-Burch,
       address={Johns Hopkins University, Baltimore}}

% The braces are mandatory only if the value contains a comma as in the first
% author's address but they never make any harm, you can safely put all
% values into braces.
% 
% The title and authors' names are used in the running head. If they are
% long, you should define short versions. These definitions are optional. You
% define them only if they are needed. The example follows:

\shorttitle{Grammar Extraction}
\shortauthor{L. Schwartz, C. Callison-Burch}

% Now print the title by:

\maketitle


% Abstract
% ========
% 
% The abstract is placed within the "abstract" environment. It is a mandatory
% part of the article.

\begin{abstract}
While example-based machine translation has long used corpus information at run-time, statistical phrase-based approaches typically include a preprocessing stage where an aligned parallel corpus is split into phrases, and parameter values are calculated for each phrase using simple relative frequency estimates. This paper describes an open source implementation of the crucial algorithms presented in \cite{Lopez2008} which allow direct run-time calculation of SCFG translation rules in Joshua.
\end{abstract}


% The body of the article
% =======================

\section{Introduction}

A significant amount of the recent research in statistical machine translation has focused on modeling translation based on contiguous strings of words, called {\it phrases}, in the source language and corresponding phrases in the target language. Phrase-based translation \cite{Och1999,Koehn2003,Marcu2002,Och2004} have proved to be very successful, and many state-of-the-art machine translation systems are based on these approaches.

A critical component in phrase-based translation is the estimation of a translation model from a word-aligned parallel text. A phrase table containing the source phrases, their target translations and their associated probabilities that  is typically extracted in a preprocessing stage before decoding a test set \cite{Koehn2003,Kumar2006}. An example of this preprocessing approach is found in the training scripts provided as part of the open source phrase-based Moses toolkit \cite{Moses}. Hierarchical phrase-based translation \cite{Chiang2005} extends phrase-based translation by allowing phrases with gaps, modeled as a synchronous context-free grammar (SCFG). The original Hiero implementation \cite{Chiang2007} trains its SFCG translation model in a similar preprocessing stage.

By contrast, example-based machine translation (EBMT) approaches \cite{Nagao1981,Sato1990,Somers2003} are notable for their use of aligned parallel corpora at run time. EBMT research has successfully explored how various efficient data structures for pattern matching \cite{Brown2004} can be leveraged to allow the decoder to access at decode-time portions of the training text that are most relevant to the text currently being translated. The Cunei machine translation toolkit \cite{Phillips2009} is an open source, statistical EBMT system that follows this approach.
 
Suffix arrays are compact data structures which allow efficient pattern matching to be performed over all text in a corpus \cite{Manber1990}.  \newcite{Callison-Burch2005b} and \newcite{Zhang2005} showed that suffix arrays can be adapted to allow phrase-based translation to calculate translation options for the translation model at run-time. A subsample of occurrences of given source phrase are used to calculate translation probabilities. By accessing the target corpus and word alignment data, the phrasal translations and their associated model parameters can be calculated at run-time. \newcite{Lopez2007} showed that hierarchical phrases can also by obtained at run time using a suffix array. 
%This work incorporated a trie-based grammar mechanism \cite{Fredkin1960}. 

This article reports on an implementation of the basic techniques described in \newcite{Lopez2008} that was incoporated into the open source machine translation system Joshua \cite{Joshua-WMT}. The implementation described here enables users of Joshua to begin translating sentences using an aligned parallel corpus without having to extract an SCFG before decoding begins. 
The advantages of using this implementation are that any input sentence can be decoded (making it appropriate for live demos or real world use), and that the data structures require much less disk space than full phrase tables.  This comes at the cost of slower running time for the decoder itself, since phrase translations have to be calculated on the fly. 
%This implementation can also be used during a preprocessing stage to extract an SCFG for a specific test set.

%The remainder of this paper is structured as follows. Section \ref{related-work} briefly overviews the grammar extraction mechanisms employed by other prominent open source machine translation systems. Section \ref{data-structures} reviews the data structures and algorithms that are used for efficient run-time calculation of SCFG rules. Finally, section \ref{using-joshua} provides details for configuring and running the Joshua grammar extraction tools.


\section{Related Work}
\label{related-work}

While example-based machine translation has long used corpus information at run-time, statistical phrase-based approaches typically include a preprocessing stage where an aligned parallel corpus is split into phrases, and parameter values are calculated for each phrase using simple relative frequency estimates.
%
The phrase-based decoders Pharaoh \cite{Koehn2004} and Moses \cite{Moses} take this approach, providing users with scripts to estimate a translation model from a sentence-aligned parallel corpus. Similarly, Hiero \cite{Chiang2007} and the syntax-augmented machine translation (SAMT) system \cite{samt2006} both require a preprocessing stage to extract a SCFG translation model. Recent work in Moses (Huang and Koehn, p.c.) provides similar functionality for extracting an SCFG-based translation model during a preprocessing stage.

\newcite{Callison-Burch2005b}, \newcite{Zhang2005}, and \newcite{Lopez2008} all describe implementations of traditional phrase-based models extended to take advantage of suffix array data structures to extract phrase translation options at run-time.  However, functional open source implementations of these have yet to be made available. Preliminary work has investigated integrating these techniques into Moses, but this work is not complete.\footnote{Much of this preliminary work was conducted by Chris Callison-Burch, Andreas Eisele, Juri Ganitkevitch, and Adam Lopez at the Second Machine Translation Marathon in 2008.}

\newcite{Lopez2008} provides a fast implementation of SCFG grammar extraction for Hiero which uses suffix arrays. This implementation allows Hiero to use an aligned parallel corpus at run-time in lieu of a pre-extracted SCFG. However, this implementation is not available as open source software due to intellectual property restrictions imposed by the University of Maryland. Cunei \cite{Phillips2009}\footnote{\url{http://sourceforge.net/projects/cunei}} is a statistical open source EBMT system that uses suffix arrays to extract relevant phrase pairs from an aligned parallel corpus at run-time. 


\section{Implementation: Data Structures and Algorithms}
\label{data-structures}



To extract hierarchical translation rules at run-time, the decoder must have access to the aligned parallel corpus. Internally, Joshua treats all source and target words as 32-bit integers. Each unique string that is encountered is assigned a unique integer. A hash map maintains the mapping from string to integer, while a corresponding list of strings maintains the mapping from integer back to string. Together these data structures comprise the {\bf symbol table}.

A corpus can be considered a simple list whose size is equal to the number of words in the corpus. Using this approach with the symbol table, Joshua stores the source corpus as an integer array. An auxiliary array, with size equal to the number of sentences in the source corpus, is maintained.  Elements in this auxiliary sentence array indicate the corpus index where each sentence begins. The target corpus is likewise represented by a {\bf corpus array} and corresponding sentence array.

\begin{figure}
\begin{algorithmic}[1]
\Function{Compare\_Elements}{$index1$,  $index2$, $max$, $corpusEnd$}
	\For {$i=0$; $i<max$; $i++$}
		\If {$index1+i < corpusEnd$ and $index2+i > corpusEnd$}
			\State \Return{$1$}
		\ElsIf {$index2+i < corpusEnd$ and $index1+i>corpusEnd$}
			\State \Return{$-1$}
		\ElsIf {$corpus[index1+i]$ is lexicographically < $corpus[index2+i]$}
			\State \Return {$-1$}
		\ElsIf {$corpus[index1+i]$ is lexicographically > $corpus[index2+i]$}
			\State \Return {$1$}
		\EndIf
	\EndFor
	\State \Return {$0$}
\EndFunction
\end{algorithmic}
\caption{During suffix array creation, the contents of a corpus array are sorted using the element comparison function {\sc {Compare\_Elements}}}
\label{sort}
\end{figure}


Once the source and target corpus arrays are available, the corresponding {\bf suffix array}s can be constructed. Given a corpus array $c$ and a symbol table, a second array is created of equal size to the corpus array. This array $s$ is initialized such that $s[x] = x$. Where each integer in $c$ represents a word string, each integer in $s$ represents an index into $c$. The contents of $s$ are sorted, using the element comparison function defined in Figure \ref{sort}. After sorting, the indices of all instances in the corpus of any given phrase are located in a contiguous segment in the suffix array $s$.

While a phrase-based decoder can simply look up any required phrase in a suffix array, hierarchical decoders must deal with discontinuous phrases that include gaps. To deal with such phrases, \namecite{Lopez2008} defines an incremental algorithm for constructing a specialized trie \cite{Fredkin1960} to represent the SCFG translation grammar. Given a source sentence, this algorithm constructs a {\bf prefix tree with suffix links} by first examining all possible contiguous source phrases, and uses the source suffix array to look up translations for contiguous phrases. Hierarchical phrases that consist of a contiguous phrase preceded or followed by a single nonterminal $X$ can be constructed directly from the corresponding contiguous phrase. In this manner, the tree is gradually constructed into a grammar containing contiguous phrases and simple hierarchical phrases. 



\begin{figure}
%\begin{algorithm}
%\caption{The basic {\sc QueryIntersect} algorithm}
\noindent {\bf Algorithm} {\sc Query\_Intersect}

\noindent {\bf Input:} Sorted list of corpus locations matching source language pattern $a\alpha$:  $M_{a\alpha}$

\noindent {\bf Input:} Sorted list of corpus locations matching source language pattern $\alpha{}b$:  $M_{\alpha{}b}$ 

\ \\ 

\begin{algorithmic}[1]
\Function{Query\_Intersect}{$M_{a\alpha}$,  $M_{\alpha{}b}$}
	\State $M_{a\alpha{}b} \leftarrow \emptyset$ \Comment{Result list is initially empty}
	\State $I \leftarrow |M_{a\alpha}|$ \Comment{Number of instances of pattern $a\alpha$ in the source corpus}
	\State $J \leftarrow |M_{\alpha{}b}|$  \Comment{Number of instances of pattern $\alpha{}b$ in the source corpus}
	\State $j \leftarrow 0$
	\State $i \leftarrow 0$
	
	\While {$i<I$ and $j<J$}
		\State \Comment{Ignore elements in $M_{\alpha{}b}$ that are}
		\State \Comment{too distant to intersect with $M_{a\alpha}[i]$}
		\While {$j<J$ and $M_{a\alpha}[i] \ddot{>} M_{\alpha{}b}[j]$}
			\State $j \leftarrow j + 1$ 
		\EndWhile
	
		\State  \Comment{Verify that the corpus index}
		\State \Comment{for the first terminal symbol}
		\State $k \leftarrow i$ \Comment{in pattern $\alpha{}b$ is the same }
		%\State \Comment{is the same as the corpus index for the first terminal symbol in pattern $\alpha{}b$ at }
		\While {$M_{\alpha{}b}[i]_{,1}$ = $M_{\alpha{}b}[k]_{,1}$} \Comment{for locations $M_{\alpha{}b}[i]$ and $M_{\alpha{}b}[k]$}
			\State $\ell \leftarrow j$
			\While {$\ell < J$ and not $M_{a\alpha}[i] \ddot{<} M_{\alpha{}b}[\ell]$}
				\If {$M_{a\alpha}[i] \ddot{=} M_{\alpha{}b}[\ell]$}
					\State {Intersect $M_{a\alpha}[i]$ with $M_{\alpha{}b}[\ell]$ and append result to  $M_{a\alpha{}b}$}
				\EndIf
				\State $\ell \leftarrow \ell + 1$ \Comment{Proceed to next element in $M_{\alpha{}b}$}
			\EndWhile
			\State {$i \leftarrow i + 1$} \Comment{Proceed to next element in $M_{a\alpha}$}
		\EndWhile
	\EndWhile
	\State \Return {$M_{a\alpha{}b}$}
\EndFunction
\end{algorithmic}

\ \\

\noindent {\bf Result:} Sorted list of corpus locations matching source language pattern $a\alpha{}b$ : $M_{a\alpha{}b}$

%\end{algorithm}
\caption{Query intersection algorithm implemented in Joshua. This algorithm is adapted from a corrected version (Lopez, p.c.) of query intersection {\protect \cite{Lopez2008}}.}
\label{query}
\end{figure}


More complex hierarchical phrases are constructed using the {\sc Query\_Intersect} function in Figure \ref{query}. This function takes two smaller phrases $a\alpha$ and $\alpha{}b$ ($a$ and $b$ represent single words and $\alpha$ represents a sequence), along with the list of indices where these phrases are located. These two lists can be efficiently processed to determine the locations where the two phrases intersect to form the more complex phrase $a\alpha{}b$. In this way, all source hierarchical phrases can be located.

Each node in the prefix tree corresponds to a unique source phrase. Each node stores the complete list of all indices in the source corpus where that node's phrase occurs. These locations are used in conjunction with the target corpus array and the word alignment data to construct SCFG translation rules.

% Caching
Ideally, once translation rules have been extracted for a given source phrase, those rules would be stored and not calculated again. Memory constraints typically dictate that not all rules are stored. Rather than storing the translation rules for a given source phrase at the corresponding node in the prefix tree, a single least-recently-used (LRU) cache is maintained. This cache maps from source phrase to the corresponding set of translation rules. 

% Memory mapping
Another technique used to save memory is the option of using memory-mapped data structures. Memory-mapped version of the corpus array, suffix array, and alignment grids data structures are implemented and used by default.




%\begin{figure}
%\begin{description}
%	\item[sourceFileName] Path to binary file containing integer-ized memory-mappable source language training corpus. When selected, the {\tt joshDir} option sets the value of this option to {\tt joshDir/source.corpus}.

%	\item[targetFileName] Path to binary file containing integer-ized memory-mappable target language training corpus. When selected, the {\tt joshDir} option sets the value of this option to {\tt joshDir/target.corpus}.

%	\item[alignmentsFileName] Path to binary file containing integer-ized memory-mappable source-to-target word alignments data. When selected, the {\tt joshDir} option sets the value of this option to {\tt joshDir/alignment.grids}. The format of this file is defined in {\tt joshua.corpus.alignment.AlignmentGrids\#writeExternal} and in the {\tt joshua.corpus.alignment.MemoryMappedAlignmentGrids} constructors.
%\end{description}
%\caption{Additional options can be configured in the {\tt extractRules} target. These options are normally set automatically as a result of setting the {\tt joshDir} option. They are listed here for completeness.}
%\end{figure}





% 
% The PBML class is modelled after the standard article class. This means
% that you can use almost everything that is allowed in articles as described
% in the textbooks of LaTeX. We support sectioning commands \section,
% \subsection and \subsubsection. In addition to standard LaTeX we support
% \subsection and \subsubsection with an empty title (i.e. just with empty
% braces {} that must not even contain a space). Such subsections will be
% automatically numbered and properly formatted.
% 
% 
% Figures and tables
% ==================
% 
% Use figure and table environments. The \caption command is redefined in
% order to comply with the graphic design.
% 
% 
% Inclusion of images and other files
% ===================================
% 
% All files required for your articles must reside in the same directory. You
% are allowed to use subdirectories only exceptionally. When loading the file
% you must use a relative path, never the full path. Load the images by
% \includegraphics and other files by \input, never use \include.
% 
% 
% Cross references
% ================
% 
% You can safely used \label, \ref and \pageref with any identifiers. These
% macros are modified in order not to collide with identifiers in other
% articles.
% 
% 
% Bibliography
% ============
% 
% You may either enter the bibliography manually, possibly making use of
% \label and \ref, or use BibTeX. The bibliography style is set
% automatically. The \cite command is also modified so that it does not
% collide with other articles. You process the bibliography by BibTeX in the
% standard way and include it by:


\section{Using Joshua}
\label{using-joshua}

Given a word-aligned parallel corpus, the first step in extracting a grammar, either at run-time or during a preprocessing stage, is to compile the memory-mappable data structures to binary files on disk. The {\tt joshua.corpus.suffix\_array.Compile} program takes four parameters: source corpus text file, target corpus text file, word alignments text file, and an output directory path. 
%This program may be run directly, or may be accessed as an Apache Ant task (see Figure \ref{ant-file}). 
The output directory, by convention, is named with the suffix {\tt {\tt .josh}}. This directory stores the binary representations of the symbol table, source and target corpus arrays, and the source and target suffix arrays. 
%
These binary files are given canonical names inside the {\tt {\tt .josh}} directory, so that the decoder can use them simply by specifying the {\tt .josh} directory in the {\tt tm\_file=$\ldots$} line of the Joshua configuration file. 

It is often useful (especially during MERT) to extract a test set specific grammar once in a preprocessing step, since that test set will be translated many times and re-extracting the grammar each time would be wasteful. To perform this task, the program {\tt joshua.prefix\_tree.ExtractRules} can be used. When run directly, this program accepts either three arguments (a compiled {\tt .josh} directory, file name for grammar to extract, and test file) or five arguments (source corpus text file, target corpus text file, word alignments text file, file name for grammar to extract, and test file). 
%This program can also be accessed as an Apache Ant task (see Figure \ref{ant-file}). 
The following subsections document the mandatory and optional parameters that can be passed to this program through the {\tt extractRules} ant task.

\subsection{Mandatory parameters}
%\begin{figure}
\begin{description}

	\item[testFile] Path to plain text file containing a source language test file. The grammar extracted by {\tt extractRules} will be all of the rules required to translate the sentences in this test file.

	\item[outputFile] Path where extracted grammar will be placed. This grammar will consist of all of the rules required to translate the sentences in the test file defined in the {\tt testFile} option.

	\item[joshDir] Path to directory containing the binary files representing memory-mappable aligned parallel corpus.

\end{description}

The following parameters may be specified instead of the {\tt joshDir} parameter.
	
\begin{description}
	\item[sourceFileName] Path to text file containing source corpus. %this value is automatically set. Otherwise, this  to  Path to binary file containing integer-ized memory-mappable source language training corpus. When selected, the {\tt joshDir} option sets the value of this option to {\tt joshDir/source.corpus}.

	\item[targetFileName] Path to text file containing target corpus. 

	%\item[targetFileName] Path to binary file containing integer-ized memory-mappable target language training corpus. When selected, the {\tt joshDir} option sets the value of this option to {\tt joshDir/target.corpus}.

	\item[alignmentsFileName]  Path to text file containing word alignment data. 

	%\item[alignmentsFileName] Path to binary file containing integer-ized memory-mappable source-to-target word alignments data. When selected, the {\tt joshDir} option sets the value of this option to {\tt joshDir/alignment.grids}. The format of this file is defined in {\tt joshua.corpus.alignment.AlignmentGrids\#writeExternal} and in the {\tt joshua.corpus.alignment.MemoryMappedAlignmentGrids} constructors.

	
\end{description}
%\caption{Mandatory options must be configured in the {\tt extractRules} target.}
%\end{figure}


\subsection{Optional parameters}
%\begin{figure}
\begin{description}

	\item[maxPhraseSpan] Defines the maximum span (in the source corpus) of any extracted SCFG rule. Default value is {\tt 10}.

	\item[maxPhraseLength] Defines the maximum number of tokens (terminals plus nonterminals) allowed in the source right-hand side of any extracted SCFG rule. Default value is {\tt 10}.

	\item[maxNonterminals] Defines the maximum number of nonterminal symbols allowed in the source side of any synchronous context-free rule extracted. Note: the number and type of nonterminals is the same in the source and target right-hand sides of a SCFG rule. Default value is {\tt 2}.

	\item[cacheSize] Maximum number of source phrases for which translation rules will be maintained in the least-recently-used (LRU) cache.

	\item[encoding] Defines the file encoding scheme of the input test file and the output grammar file. Default value is {\tt UTF-8}.


	\item[ruleSampleSize] When extracting SCFG rules for a given source language phrase, this option defines the number of instances of that source phrase will be sampled from the source training corpus for use in rule extraction. Default value is {\tt 300}.
	
	\item[startingSentence] Defines the (1-based) sentence index in the test file where grammar extraction will begin. Default value is 1, indicating that a grammar will be extracted capable of translating sentences starting with the first sentence in the test file.

	\item[maxTestSentences] Defines the number of sentences in the test file over which grammar extraction will be performed. Default value is {\tt Integer.MAX\_VALUE}. For example, given a test file of 100 sentences, the options {\tt startingSentence="51"  maxTestSentences="25"} would cause grammar extraction to be performed over test sentences 51--75.

\end{description}
%\caption{Options can be configured in the {\tt extractRules} target.}
%\end{figure}


The following parameters can be configured in the {\tt extractRules} target to make rule extraction behave like the Hiero suffix array rule extractor {\protect \cite{Lopez2008} } instead of the behaving according to the rule extraction originally defined in {\protect \newcite{Chiang2005}}.

%\begin{figure}
\begin{description}

	\item[sentenceInitialX] Boolean option indicates whether rules with an initial source-side nonterminal should be extracted from phrases at the start of a sentence, even though such rules do not have supporting corporal evidence. This option is provided for compatibility with Hiero's suffix array rule extractor \cite{Lopez2008}, in which this setting is set to true. Default value is {\tt true}.

	\item[sentenceFinalX] Boolean option indicates whether rules with a final source-side nonterminal should be extracted from phrases at the end of a sentence, even though such rules do not have supporting corporal evidence. This option is provided for compatibility with Hiero's suffix array rule extractor \cite{Lopez2008}, in which this setting is set to true. Default value is {\tt true}.

	\item[edgeXViolates] Boolean option indicates whether rules with an initial or final source-side nonterminal should be extracted when the source corpus phrase span for the rule, discounting the initial or final nonterminal, is already equal to the maximum phrase span value. Since nonterminals conceptually correspond to elided elements in the training corpus, setting this value to true allows phrases which have a longer phrase span than the maximum allowed phrase span. This option is provided for compatibility with Hiero's suffix array rule extractor \cite{Lopez2008}, in which this setting is set to true. Default value is {\tt true}.

	\item[requireTightSpans] Boolean option; if true, follow the heuristic from \cite{Chiang2005}: where multiple initial phrase pairs contain the same set of alignment points, consider only the smallest when performing rule extraction. For compatibility with \newcite{Lopez2008}, set this parameter to false. Default value is {\tt true}.

\end{description}
%\caption{Options can be configured in the {\tt extractRules} target to make rule extraction behave like the Hiero suffix array rule extractor {\protect \cite{Lopez2008} } instead of the behaving according to the rule extraction originally defined in {\protect \newcite{Chiang2005}} .}
%\end{figure}

Additional options can be configured in the {\tt extractRules} target to change the behavior of the prefix tree.

%\begin{figure}
\begin{description}
	\item[keepTree] Boolean option indicates whether the prefix tree should persist from sentence to sentence during grammar extraction. If set to false, a new prefix tree will be created to process each sentence in the test file. Default value is {\tt true}.

	\item[printPrefixTree] Boolean option indicates whether a representation of the prefix tree should be printed to standard output. If set to true, the tree will be printed after processing each sentence in the test file. Default value is {\tt false}.

\end{description}
%\caption{Additional options can be configured in the {\tt extractRules} target to affect behavior of the prefix tree.}
%\end{figure}


\section*{Acknowledgements}

This research was supported in part by the EuroMatrixPlus project funded by
the European Commission under the Seventh Framework Programme, by the US National Science Foundation under grant IIS-0713448, and by the DARPA GALE program under contract number HR0011-06-2-0001.  Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.

Special thanks to Adam Lopez for his help and advice, and for making the \LaTeX code for his algorithms available. 


\bibliography{../bibliography}


% End of the article
% ==================
% 
% The article must end with:

\end{document}




\begin{figure}[hptb]
\begin{verbatimtab}[4]
<project name="Joshua Grammar Extraction in Ant" default="extract">
    
	<property name="classes.dir" value="bin"/>
	<property name="experiment.dir" value="/path/to/my/data"/>
 	
	<taskdef name="extractRules" 
		classname="joshua.prefix_tree.ExtractRules" 
		classpath="${classes.dir}"/>
	
	<taskdef name="compileJosh" 
		classname="joshua.corpus.suffix_array.Compile" 
		classpath="${classes.dir}"/>
	
	<target name="compile" 
		description="Compile aligned parallel corpus
			 into memory-mapped files">
		
		<compileJosh
			sourceCorpus="${experiment.dir}/source.txt"
			targetCorpus="${experiment.dir}/target.txt"
			alignments="${experiment.dir}/align.txt"
			outputDir="${experiment.dir}/model{\tt .josh}"
		/>
	</target>
	
	<target name="extract" 
		description="Extract a translation grammar 
			from a compiled parallel corpus">
		
		<extractRules 
			joshDir="${experiment.dir}/model{\tt .josh}" 
			testFile="${experiment.dir}/target-test.txt" 
			outputFile="${experiment.dir}/test-set-specific.grammar"
		/>    
	</target>
	
</project>
\end{verbatimtab}
\caption{Sample Apache Ant build file defining how to extract a translation grammar from an aligned parallel corpus. This example assumes that the files {\tt source.txt}, {\tt target.txt}, {\tt align.txt} and {\tt target-test.txt} already exist in the directory specified in the {\tt experiment.dir} variable. Running ant with this file will create the directory {\tt model{\tt .josh}} and extract {\tt test-set-specific.grammar}.}
\label{ant-file}
\end{figure}
