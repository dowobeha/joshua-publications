%
% File acl2010.tex
%
% Contact  jshin@csie.ncnu.edu.tw or pkoehn@inf.ed.ac.uk
%%
%% Based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xspace}

%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\newcommand{\joshua}{\textbf{Joshua}\xspace}
\newcommand{\Q}{\mathcal{Q}}

\title{Joshua 2.0: A Toolkit for Parsing-Based Machine Translation \\with Syntax, Semirings, Discriminative Training and Other Goodies }

\author{
Zhifei Li,\,\,\,
Chris Callison-Burch,\,\,\,
Chris Dyer,$^\dagger$\,\,\,
Juri Ganitkevitch,\,\,\,
\\ {\bf
Ann Irvine,\,\,\,  
Sanjeev Khudanpur,\,\,\,
Lane Schwartz,$^\star$\,\,\, 
Wren N.\,G.\,Thornton,\,\,\,}
\\ {\bf
Ziyuan Wang,\,\,
Jonathan Weese\,\,
{\textnormal{and}}
\,\,\,Omar F. Zaidan
}\\
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD\\
$\dagger$ Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD\\
$\star$ Natural Language Processing Lab, University of Minnesota, Minneapolis, MN }


\date{}

\begin{document}


\maketitle
\begin{abstract}
We describe the progress we have made in the past year on \textbf{Joshua} \cite{joshua-wmt09}, 
an open source toolkit for parsing based machine translation.
The new functionality includes: 
support for translation grammars with a rich set of syntactic nonterminals,
the ability for external modules to posit constraints on how spans in the input sentence should be translated,  
lattice parsing for dealing with input uncertainty,
a semiring framework that provides a unified way of doing various dynamic programming calculations, 
variational decoding for approximating the intractable MAP decoding,
hypergraph-based discriminative training for better feature engineering, 
a parallelized MERT module,
document-level and tail-based MERT,
visualization of the derivation trees, 
and a cleaner pipeline for MT experiments.
\end{abstract}



\section{Introduction}

\joshua is an open-source toolkit for parsing-based machine translation that is
written in Java. The initial release of \joshua  \cite{joshua-wmt09} was
a re-implementation of the Hiero system \cite{hiero-2007} and all its
associated algorithms, including: chart parsing, $n$-gram language model
integration, beam and cube pruning, and $k$-best extraction.  The \joshua 1.0
release also included re-implementations of suffix array grammar extraction
\cite{lopez:2007:EMNLP-CoNLL2007,PBML-2010-Joshua-grammar-extraction} and
minimum error rate training \cite{och-mert,Zaidan2009}.  Additionally, it
included parallel and distributed computing techniques for salability \cite{Joshua-old}. 

This paper describes the additions to the toolkit over the past year, which
together form the 2.0 release. The software has been heavily used by the
authors and several other groups in their daily research, and has been
substantially refined since the first release.  The most important new
functions in the toolkit are:
\begin{itemize}
\item
Support for any style of synchronous context free grammar (SCFG)
% instead of just Hiero grammars.  This 
including syntax augment machine translation (SAMT) grammars \cite{samt2006}
\item
Support for external modules to posit translations for spans in the input sentence that constrain decoding   \cite{PBML-2010-Joshua-modules}
\item
Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters  \cite{dyer-muresan-resnik:2008:ACLMain}
\item
A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly \cite{li-eisner:2009:EMNLP}
\item
Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding \cite{variational-decoding-acl09}
\item
Hypergraph-based discriminative training for better feature engineering \cite{zhifei-forest-reranking-galebook}
\item
A parallelization of MERT's computations, and supporting document-level and tail-based optimization \cite{atypical-mert}
\item
Visualization of the derivation trees and hypergraphs \cite{PBML-2010-Joshua-visualization}
\item
A convenient framework for designing and running reproducible machine translation experiments \cite{Schwartz-wmt10-pipline}
\end{itemize}
The sections below give short descriptions for each of these new functions.


\section{Support for Syntax-based Translation}

The initial release of \joshua supported only Hiero-style SCFGs, which use a single nonterminal symbol X.  This release includes support for arbitrary SCFGs, including ones that use a rich set of linguistic nonterminal symbols.  
In particular we have added support for \newcite{samt2006}'s syntax-augmented machine
translation. SAMT grammar extraction is identical to Hiero grammar extraction, except that one side of the parallel is parsed, and syntactic labels replace the X
nonterminals in Hiero-style rules.  Instead of extracting this Hiero rule from the bitext

\noindent
\begin{tabular}{c}
  {\small \tt [X]} $\Rightarrow$ {\small \tt [X,1]} sans {\small \tt [X,2]} 
  $\vert$ {\tt \small [X,1]} without {\small \tt [X,2]}
\end{tabular}

\noindent
the nonterminals can be labeled according to
which constituents cover the nonterminal span on the parsed side of
the bitext.  This constrains what types of phrases the decoder can use when producing a translation.

\noindent
\begin{tabular}{c}
  {\small \tt [VP]} $\Rightarrow$ {\small \tt [VBN]} sans {\small
    \tt [NP]} $\vert$ {\small \tt [VBN]} without {\small \tt [NP]} \\
  {\small \tt [NP]} $\Rightarrow$ {\small \tt [NP]} sans {\small
    \tt [NP]} 
  $\vert$ {\small \tt [NP]} without {\small \tt [NP]} 
\end{tabular}

\noindent
Unlike GHKM \cite{Galley2004}, SAMT has the same coverage as Hiero, because it allows non-constituent phrases to get syntactic
labels using CCG-style slash notation. Experimentally, we have found that the derivations created using syntactically motivated grammars exhibit more coherent syntactic structure than Hiero and typically result in better reordering, especially for languages with word orders that diverge from English, like Urdu \cite{SCALE-report}.


\section{Specifying Constraints on Translation}

Integrating output from specialized modules (like transliterators,
morphological analyzers, and modality translators) into the MT pipeline can
improve translation performance, particularly for low-resource languages. We
have implemented an XML interface that allows external modules to propose
alternate translation rules (constraints) for a particular word span to the
decoder \cite{PBML-2010-Joshua-modules}. Processing that is separate from the
MT engine can suggest translations for some set of source side words and
phrases. The XML format allows for both hard constraints, which must be used,
and soft constraints, which compete with standard extracted translation rules,
as well as specifying associated feature weights. In addition to specifying
translations, the XML format allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a particular span to be translated
as an NP.  We modified \joshua's chart-based decoder to support these
constraints. 


\section{Semiring Parsing}

In \joshua, we use a hypergraph (or packed forest) to compactly 
represent the exponentially many derivation trees generated by the decoder for an input sentence.
Given a hypergraph, we may perform many atomic inference operations,
such as finding one-best or $k$-best translations, or computing expectations over the hypergraph.
For each such operation, we could implement a dedicated dynamic programming algorithm.
However, a more general framework to specify these algorithms is semiring-weighted parsing \cite{semiringparsing}.
We have implemented  the inside algorithm, the outside algorithm, and the inside-outside speedup described by \newcite{li-eisner:2009:EMNLP},
plut the first-order expectation semiring \cite{eisner-expectation-semiring} and its
second-order version \cite{li-eisner:2009:EMNLP}.  All of these use our newly implemented semiring framework.

% The original first-order expectation semiring allows us to efficiently
% compute a vector of first-order statistics (expectations; first
% derivatives) on the set of paths in a lattice or the set of trees in a
% hypergraph.  The second-order expectation semiring {\em additionally} computes
% a matrix of second-order statistics (expectations of {\em products}; second
% derivatives (Hessian); derivatives of expectations).

%% use of expectation and variance semiring
The first- and second-order expectation semi-rings can also be used to compute many interesting quantities over hypergraphs.
These quantities include expected translation length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence,
Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on.

% The second-order expectation semiring is essential for many interesting training paradigms 
% such as deterministic annealing \cite{Rose98deterministicannealing}, minimum risk \cite{smith-eisner:2006:POS},
% active and semi-supervised learning \cite{entropy-mini,semiCRF-jiao}.
% In these settings, we must compute the gradient of entropy or risk. 
% The semiring can also be used for second-order gradient optimization algorithms.


\section{Word Lattice Input}

We generalized the bottom-up parsing algorithm that generates the translation
hypergraph so that it supports translation of word lattices instead of just
sentences.  Our implementation's runtime and memory overhead is proportional to
the size of the lattice, rather than the number of paths in the lattice
\cite{dyer-muresan-resnik:2008:ACLMain}.  Accepting lattice-based input allows
the decoder to explore a distribution over input sentences,  allowing it to
select the best translation from among all of them.  This is especially useful
when \joshua is used to translate the output of statistical preprocessing
components, such as speech recognizers or Chinese word segmenters, which can
encode their alternative analyses as confusion networks or lattices. 


\section{Variational Decoding}

Statistical models in machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations) that have the same yield. In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
during decoding is then NP-hard.
The first version of \joshua implemented the
Viterbi approximation, which measures the goodness 
of a translation using only its most probable derivation.

The Viterbi approximation is efficient, but it ignores most of the derivations in the hypergraph.
We implemented variational decoding \cite{variational-decoding-acl09}, which works as follows.
First, given a foreign string (or lattice), the MT system produces a hypergraph, 
which encodes a probability distribution $p$ over 
possible output strings and their derivations.
Second, a distribution $q$ is selected that approximates $p$ as well as possible but comes from a family of distributions $\mathcal{Q}$ in which inference is tractable.  Third, the best string according to $q$ (instead of $p$) is found.
In our implementation, the $q$ distribution is parameterized by an $n$-gram model, under which 
the second and third steps can be performed efficiently and exactly via dynamic programming.
In this way, variational decoding considers all derivations in the hypergraph but still
allows tractable decoding. 


\section{Hypergraph-based Discriminative Training}

Discriminative training with a large number of features has 
potential to improve the MT performance.
We have implemented the hypergraph-based minimum risk training \cite{li-eisner:2009:EMNLP},
which minimizes the {\em expected loss} of the reference translations.
The minimum-risk objective can be optimized by a gradient-based method, where
the risk and its gradient can be computed using a second-order expectation semiring.
For optimization, we use both L-BFGS\footnote{http://en.wikipedia.org/wiki/L-BFGS} 
and Rprop\footnote{http://en.wikipedia.org/wiki/Rprop}.

We have also implemented the average Perceptron algorithm and forest-reranking \cite{zhifei-forest-reranking-galebook}.
Since the reference translation may not be in the hypergraph due to pruning or inherent
defficiency of the translation grammar, we need to use an {\em oracle translation} (i.e., the translation in
the hypergraph that is most simmilar to the reference translation) as a surrogate for training.
We implemented the {\em oracle extraction} algorithm described by \newcite{oracle-extraction-naacl09}
for this purpose.

Given the current infrastructure, other training methods 
(e.g., maximum conditional likelihood or MIRA as used by \newcite{chiang-knight-wang:2009:NAACLHLT09})
can also be easily supported with minimum coding.
We plan to implement a large number of feature functions in \joshua so that exhaustive 
feature engineering is possible for MT.


\section{Minimum Error Rate Training}

\joshua's MERT module optimizes parameter weights so as to maximize performance
on a development set as measuered by an automatic evaluation metric, such as
Bleu \cite{och-mert}.

We have parallelized our MERT module in two ways: parallelizing the computation
of metric scores, and parallelizing the search over parameters. The computation
of metric scores is a computational concern when tuning to a metric that is
slow to compute, such as translation edit rate \cite{snover-etal:2006:ter}.
Since scoring a candidate is independent from scoring any other candidate, we
parallelize this computation using a multi-threaded solution\footnote{Based on
sample code by Kenneth Heafield.}. Similarly, we parallelize the optimization
of the intermediate initial weight vectors, also using a multi-threaded
solution.

Another feature is the module's awareness of document information, and the
capability to perform optimization of {\em document-based} variants of the
automatic metric \cite{atypical-mert}. For example, in document-based Bleu,
a Bleu score is calculated {\em for each document}, and the tuned score is the
average of those document scores. The MERT module can furthermore be instructed
to target a specific subset of those documents, namely the {\em tail} subset,
where only the subset of documents with the lowest document Bleu scores are
considered.\footnote{This feature is of interest to GALE teams, for instance,
since GALE's evaluation criteria place a lot of focus on translation quality of
tail documents.}

More details on the MERT method and the
implementation can be found in \newcite{Zaidan2009}.\footnote{The module
is also available as a standalone application, {\em Z-MERT}, that can be used
with other MT systems. (Software and documentation at: \url{http://cs.jhu.edu/~ozaidan/zmert}.)}


\section{Visualization}

We created tools for visualizing two of the main data structures used
in \joshua \cite{PBML-2010-Joshua-visualization}. The first visualizer displays hypergraphs. The user can choose from
a set of input sentences, then call the decoder to build the hypergraph.
The second  visualizer displays derivation trees. Setting a flag in the configuration file causes the decoder to output parse trees instead of strings, where each nonterminal is annotated with its source-side
span. The visualizer can read in multiple n-best lists in this format, then
display the resulting derivation trees side-by-side. We have found that
visually inspecting these derivation trees is useful for debugging
grammars.

We would like to add visualization tools for more parts of the pipeline. For
example, a chart visualizer would make it easier for researchers to tell where
search errors were happening during decoding, and why. An alignment visualizer
for aligned parallel corpora might help to determine how grammar extraction 
could be improved.


\section{Pipeline for Running MT Experiments}

Reproducing other researchers' machine translation experiments is difficult
because the pipeline is too complex to fully detail in short conference papers.
We have put together a workflow framework for designing and running
reproducible machine translation experiments using \joshua \cite{Schwartz-wmt10-pipline}.
Each step in the machine translation workflow (data preprocessing, grammar
training, MERT, decoding, etc) is modeled by a Make script that defines how to
run the tools used in that step, and an auxiliary configuration file that
defines the exact parameters to be used in that step for a particular
experimental setup. Workflows configured using this framework allow a complete
experiment to be run -- from downloading data and software through scoring the
final translated results -- by executing a single Makefile.

This framework encourages researchers to supplement research publications with
links to the complete set of scripts and configurations that were actually used
to run the experiment. The Johns Hopkins University submission for the WMT10
shared translation task was implemented in this framework, so it can be easily
and exactly reproduced.


\section*{Acknowledgements}

Research funding was provided by the NSF under grant IIS-0713448, by the European Commission through the EuroMatrixPlus project, and by the DARPA GALE program under Contract No. HR0011-06-2-0001. The views and findings are the authors' alone.


\bibliographystyle{acl}
\bibliography{machinetranslation}

\end{document}
