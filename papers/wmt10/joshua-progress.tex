%
% File acl2010.tex
%
% Contact  jshin@csie.ncnu.edu.tw or pkoehn@inf.ed.ac.uk
%%
%% Based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xspace}

%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\newcommand{\joshua}{\textbf{Joshua}\xspace}
\newcommand{\Q}{\mathcal{Q}}

\title{Joshua 2.0: A Toolkit for Parsing-Based Machine Translation \\with Syntax, Semirings, Discriminative Training and Other Goodies }

\author{
Zhifei Li,\,\,\,
Chris Callison-Burch,\,\,\,
Chris Dyer,$^\dagger$\,\,\,
Juri Ganitkevitch,\,\,\,
\\ {\bf
Ann Irvine,\,\,\,  
Sanjeev Khudanpur,\,\,\,
Lane Schwartz,$^\star$\,\,\, 
Wren N.\,G.\,Thornton,\,\,\,}
\\ {\bf
Ziyuan Wang,\,\,
Jonathan Weese\,\,
{\textnormal{and}}
\,\,\,Omar F. Zaidan
}\\
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD\\
$\dagger$ Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD\\
$\star$ Natural Language Processing Lab, University of Minnesota, Minneapolis, MN }


\date{}

\begin{document}


\maketitle
\begin{abstract}
We describe the progress we have made in the past year on \textbf{Joshua} \cite{joshua-wmt09}, 
an open source toolkit for parsing based machine translation.
The new functionality includes: 
support for translation grammars with a rich set of syntactic nonterminals,
the ability for external modules to posit constraints on spans in the input sentence should be translated,  
lattice parsing for dealing with input uncertainty,
a semiring framework that provides a unified way of doing various dynamic programming calculations, 
variational decoding for approximating the intractable MAP decoding,
hypergraph-based discriminative training for better feature engineering, 
visualization of the derivation trees, 
and a cleaner pipeline for MT experiments.
\end{abstract}



\section{Introduction}

\textbf{Joshua} \cite{joshua-wmt09} is an open-source toolkit for parsing-based machine translation (written in Java). 
The first version implemented all the essential algorithms described 
in \newcite{hiero-2007}: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and $k$-best extraction.  
It also implemented suffix-array grammar extraction \cite{lopez:2007:EMNLP-CoNLL2007} 
and minimum error rate training \cite{och-mert}. 
Additionally, parallel and distributed computing techniques were exploited 
to make it scalable \cite{Joshua-old}. In the design of \joshua, we have applied general
principles of software engineering to achieve three
major goals: extensibility, end-to-end coherence,
and scalability.

The toolkit has been used by the authors and several
other groups in their daily research, and thus much progress has
been made to the first version. 
In this paper, we describe the main new functions we have added into the 
toolkit, which include: 
\begin{itemize}
\item
support for the syntax-augmented machine translation (SAMT) grammar \cite{samt2006},
\item
support for external modules to posit translations for spans in the input sentence that constrain decoding,    \cite{PBML-2010-Josua-transliteration},  
\item
lattice parsing for dealing with input uncertainty \cite{dyer-muresan-resnik:2008:ACLMain},
\item
semiring parsing for unified dynamic programming \cite{li-eisner:2009:EMNLP} , 
\item
variational decoding for approximating the intractable MAP decoding \cite{variational-decoding-acl09},
\item
hypergraph-based discriminative training for better feature engineering \cite{zhifei-forest-reranking-galebook}, 
\item
visualization of the derivation trees and hypergraphs \cite{PBML-2010-Josua-visualization}, 
\item
and a unified framework for designing and running reproducible machine translation experiments \cite{Schwartz-wmt10-pipline}.
\end{itemize}
Below we give a short description for each of the new functions added.


\section{Support for Syntax Augmented MT}
\todo{juri or ccb}


\section{Specifying Constraints on Rules}
\todo{anni}

\section{Semiring Parsing}

In \joshua, we use a {\em hypergraph} (or {\em packed forest}) to compactly 
represent the exponentially many derivation trees generated by the decoder for a 
particular foreign input sentence.
Given a hypergraph, we may perform many atomic inference operations
such as finding one-best, $k$-best, or expectations over the hypergraph.
To perform each operation, we may implement a dedicated dynamic programming algorithm.
However, a more general framework to specify these algorithms is semiring-weighted parsing \cite{semiringparsing}.
Within this framework, we implemented all the algorithms (i.e.,
inside, outside, and inside-outside speedup) described by \newcite{li-eisner:2009:EMNLP},
and also the first-order expectation semiring \cite{eisner-expectation-semiring} and its
second-order version \cite{li-eisner:2009:EMNLP}.

% The original first-order expectation semiring allows us to efficiently
% compute a vector of first-order statistics (expectations; first
% derivatives) on the set of paths in a lattice or the set of trees in a
% hypergraph.  The second-order expectation semiring {\em additionally} computes
% a matrix of second-order statistics (expectations of {\em products}; second
% derivatives (Hessian); derivatives of expectations).

%% use of expectation and variance semiring
The first- and second-order expectation semirings can be used compute many interesting quantities over the hypergraph.
These quantities include expected hypothesis length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence,
Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on.

% The second-order expectation semiring is essential for many interesting training paradigms 
% such as deterministic annealing \cite{Rose98deterministicannealing}, minimum risk \cite{smith-eisner:2006:POS},
% active and semi-supervised learning \cite{entropy-mini,semiCRF-jiao}.
% In these settings, we must compute the gradient of entropy or risk. 
% The semiring can also be used for second-order gradient optimization algorithms.

\section{Lattice Parsing}

The bottom-up parsing algorithm used to generate the translation hypergraph has been generalized to support translation from not only an input sentence but from a \emph{word lattice} with runtime and memory overhead that is proportional to the size of the lattice, rather than the number of paths in the lattice \cite{dyer-muresan-resnik:2008:ACLMain}.  This enables the exploration of a distribution over input sentences during decoding, and the best translation is selected from among any of them.  This functionality is useful when \joshua is used to translate the output of statistical preprocessing components, such as speech recognizers or word segmenters.

\section{Variational Decoding}

Statistical models in machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
during decoding is then NP-hard.
The first version of \joshua implemented the
Viterbi approximation that measures the goodness 
of a string using only its most probable derivation.

The Viterbi approximation is efficient, but it ignores most of the derivations in the hypergraph.
We implemented variational decoding \cite{variational-decoding-acl09}, which works as follows.
First, given a foreign string (or lattice), the MT system produces a hypergraph, 
which encodes a probability distribution $p$ over 
possible output strings and their derivations.
Second, a distribution $q$ is selected that approximates $p$ as well as possible but comes from a family of distributions $\mathcal{Q}$ where inference is tractable.  Third, the best string according to $q$ (instead of $p$) is found.
In our implementation, the $q$ distribution is parameterized by an $n$-gram model, under which 
the second and third steps can be performed efficiently and exactly via dynamic programming.
In this way, variational decoding considers all derivations in the hypergraph but still
allows tractable decoding. 



\section{Hypergraph-based Discriminative Training}

Discriminative training with a large number of features has 
potential to improve the MT performance.
We have implemented the hypergraph-based minimum risk training \cite{li-eisner:2009:EMNLP},
which minimizes the {\em expected loss} of the reference translations.
The minimum-risk objective can be optimized by a gradient-based method, where
the risk and its gradient can be computed using a second-order expectation semiring.
For optimization, we use both L-BFGS\footnote{http://en.wikipedia.org/wiki/L-BFGS} 
and Rprop\footnote{http://en.wikipedia.org/wiki/Rprop}.

We have also implemented the average Perceptron algorithm and forest-reranking \cite{zhifei-forest-reranking-galebook}.
Since the reference translation may not be in the hypergraph due to pruning or inherent
defficiency of the translation grammar, we need to use an {\em oracle translation} (i.e., the translation in
the hypergraph that is most simmilar to the reference translation) as a surrogate for training.
We implemented the {\em oracle extraction} algorithm described by \newcite{oracle-extraction-naacl09}
for this purpose.

Given the current infrastructure, other training methods 
(e.g., maximum conditional likelihood or MIRA as used by \newcite{chiang-knight-wang:2009:NAACLHLT09})
can also be easily supported with minimum coding.
We plan to implement a large number of feature functions in \joshua so that exhaustive 
feature engineering is possible for MT.

\section{Visualization}

We have introduced tools for visualizing two of the main data structures used
in Joshua \cite{PBML-2010-Josua-visualization}. Each visualization tool is implemented as a
stand-alone program that hooks into the pipeline, but they are distributed with
Joshua. The first program is a hypergraph visualizer. The user can choose from
a set of input sentences, then call the decoder to build the hypergraph.

The second is a derivation-tree visualizer. Joshua can produce output formatted
as a parse tree, where each nonterminal has been annotated with its source-side
span. The visualizer can read in multiple n-best lists in this format, then
display the resulting derivation trees side-by-side. We have found that
visually inspecting these derivation trees is advantageous in debugging many
grammars.

We would like to add visualization tools for more parts of the pipeline. For
example, a chart visualizer would make it easier for researchers to tell where
search errors were happening during decoding, and why. An alignment visualizer
for aligned parallel corpora might help to determine how grammar extraction 
could be improved.


\section{MT Pipeline}

Many research publications describe novel techniques and provide interesting results, yet fail to describe technical details in sufficient detail to allow their results to be reproduced by other researchers. Reproducibility in machine translation is made more challenging by the complexity of experimental workflows. Results in machine translation tasks are dependent on a cascade of processing steps and configurations. 

\newcite{Schwartz-wmt10-pipline} describes a workflow framework for designing and running reproducible machine translation experiments using Joshua. Each step in the machine translation workflow (data preprocessing, grammar training, MERT, decoding, etc) is modeled by a GNU Make script which defines how to run the tools used in that step, and an auxiliary configuration file that defines the exact parameters to be used in that step during a particular run of an experiment. Workflows configured using this framework allow a complete experiment to be run - from downloading data and software through truecasing translated results - by executing a single make file.

This framework encourages researchers to supplement research publications with links to the complete set of scripts and configurations that were actually used to run the experiment. The scripts and configurations used to run the Johns Hopkins University submission to the 2010 WMT shared translation task are provided in \newcite{Schwartz-wmt10-pipline}.

\bibliographystyle{acl}
\bibliography{machinetranslation}

\end{document}
