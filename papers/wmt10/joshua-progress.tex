%
% File acl2010.tex
%
% Contact  jshin@csie.ncnu.edu.tw or pkoehn@inf.ed.ac.uk
%%
%% Based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xspace}

%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\newcommand{\joshua}{\textbf{Joshua}\xspace}
\newcommand{\Q}{\mathcal{Q}}

\title{Joshua 2.0: A Toolkit for Parsing-Based Machine Translation \\with Syntax, Semirings, Discriminative Training and Other Goodies }

\author{
Zhifei Li,\,\,\,
Chris Callison-Burch,\,\,\,
Chris Dyer,$^\dagger$\,\,\,
Juri Ganitkevitch,\,\,\,
\\ {\bf
Ann Irvine,\,\,\,  
Sanjeev Khudanpur,\,\,\,
Lane Schwartz,$^\star$\,\,\, 
Wren N.\,G.\,Thornton,\,\,\,}
\\ {\bf
Ziyuan Wang,\,\,
Jonathan Weese\,\,
{\textnormal{and}}
\,\,\,Omar F. Zaidan
}\\
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD\\
$\dagger$ Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD\\
$\star$ Natural Language Processing Lab, University of Minnesota, Minneapolis, MN }


\date{}

\begin{document}


\maketitle
\begin{abstract}
We describe the progress we have made in the past year to \textbf{Joshua} \cite{joshua-wmt09}, 
an open-source toolkit for parsing-based machine translation.
The main new functions we have added into the toolkit include: supporting for the syntax-augmented machine translation (SAMT) grammar,
supporting mannual constraints in the input to constrain the decoding,  
semiring parsing for unified dynamic programming, hypergraph-based 
discriminative training for better feature engineering, visualization of the derivation
trees and hypergraphs, and a unified pipeline for MT experiments.
\end{abstract}


\listoftodos

\section{Introduction}

\textbf{Joshua} \cite{joshua-wmt09} is an open-source toolkit (written in Java) for parsing-based machine translation.
The first version implemented all the essential algorithms described 
in \newcite{hiero-2007}: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and $k$-best extraction.  
The initial release also implemented suffix-array grammar extraction \cite{lopez:2007:EMNLP-CoNLL2007} 
and minimum error rate training \cite{och-mert}. 
Additionally, parallel and distributed computing techniques were exploited 
to make it scalable \cite{Joshua-old}. In the design of \joshua, we have applied general
principles of software engineering to achieve three
major goals: extensibility, end-to-end coherence,
and scalability.

The toolkit has then been used by the authors and several
other groups in their daily research, and thus much progress has
been made to the first version. 
In this report, we will describe the main new functions we have added into the 
toolkit, which include: supporting for the syntax-augmented machine translation (SAMT) grammar \cite{samt2006},
supporting manual constraints in the input to constrain the decoding \cite{PBML-2010-Josua-transliteration},  
semiring parsing for unified dynamic programming \cite{li-eisner:2009:EMNLP}, 
hypergraph-based discriminative training for better feature engineering \cite{oracle-extraction-naacl09}, 
visualization of the derivation trees and hypergraphs \cite{PBML-2010-Josua-visualization}, 
and a unified pipeline for MT experiments \cite{Schwartz-wmt10-pipline}.

Below we give a short description for each of the new functions added.


\section{Support for Syntax Augmented MT}
\todo{juri or ccb}


\section{Manual Constraints on the Input}
\todo{anni}

\section{Lattice Parsing}
\todo{Chris Dyer}


\section{Semiring Parsing}

In \joshua, we use a {\em hypergraph} (or {\em packed forest}) to compactly 
represent the exponentially many derivation trees generated by the decoder for a 
particular foreign input sentence.
Given a hypergraph, we may perform many atomic inference operations
such as finding one-best, $k$-best, or expectations over the hypergraph.
To perform each operation, we may implement a dedicated dynamic programming algorithm.
However, a more general framework to specify these algorithms is semiring-weighted parsing \cite{semiringparsing}.
Within this framework, we implemented all the algorithms (i.e.,
inside, outside, and inside-outside speedup) described by \newcite{li-eisner:2009:EMNLP},
and also the first-order expectation semiring \cite{eisner-expectation-semiring} and its
second-order version \cite{li-eisner:2009:EMNLP}.

% The original first-order expectation semiring allows us to efficiently
% compute a vector of first-order statistics (expectations; first
% derivatives) on the set of paths in a lattice or the set of trees in a
% hypergraph.  The second-order expectation semiring {\em additionally} computes
% a matrix of second-order statistics (expectations of {\em products}; second
% derivatives (Hessian); derivatives of expectations).

%% use of expectation and variance semiring
The first- and second-order expectation semirings can be used compute many interesting quantities over the hypergraph.
These quantities include expected hypothesis length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence,
Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on.

% The second-order expectation semiring is essential for many interesting training paradigms 
% such as deterministic annealing \cite{Rose98deterministicannealing}, minimum risk \cite{smith-eisner:2006:POS},
% active and semi-supervised learning \cite{entropy-mini,semiCRF-jiao}.
% In these settings, we must compute the gradient of entropy or risk. 
% The semiring can also be used for second-order gradient optimization algorithms.

\section{Variational Decoding}

Statistical models in machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then NP-hard.
The first version of \joshua implemented the
Viterbi approximation that measures the goodness 
of a string using only its most probable derivation.

The Viterbi approximation is efficient, but ignores most of the derivations in the hypergraph.
We implemented the variational decoding \cite{variational-decoding-acl09}, which works as follows.
Given a foreign string, the MT system first produces a hypergraph, 
which encodes a probability distribution $p$ over 
possible output strings and their derivations.
The variational decoding constructs a second distribution $q \in \Q$ 
that approximates $p$ as well as possible, 
and then finds the best string according to $q$ (instead of $p$).
The $q$ distribution is parameterized by an $n$-gram model, under which 
the last two steps can be performed efficiently via dynamic programming.
In this way, variational decoding considers all the derivations but still
allows tractable decoding. 



\section{Hypergraph-based Discriminative Training}

Discriminative training with a large number of features has 
potential to improve the MT performance.
We have implemented the hypergraph-based minimum risk training \cite{li-eisner:2009:EMNLP},
which minimizes the {\em expected loss} of the reference translations.
The minimum-risk objective can be optimized by a gradient-based method, where
the risk and its gradient can be computed using a second-order expectation semiring.
For optimization, we use both L-BFGS\footnote{http://en.wikipedia.org/wiki/L-BFGS} 
and Rprop\footnote{http://en.wikipedia.org/wiki/Rprop}.

We have also implemented the average Peceptron algorithm and the forest-reranking \cite{zhifei-forest-reranking-galebook}.
Since the reference translation may not be in the hypergraph due to pruning or inherent
defficiency of the translation grammar, we need to use an {\em oracle translation} (i.e., the translation in
the hypergraph that is most simmilar to the reference translation) as a surrogate for training.
We implemented the {\em oracle extraction} algorithm described by \newcite{oracle-extraction-naacl09}
for this purpose.

Given the current infrastructure, other training methods 
(e.g., maximum conditional likelihood or MIRA as used by \newcite{chiang-knight-wang:2009:NAACLHLT09})
can also be easily supported with minimum coding.
We plan to implement a large number of feature functions in \joshua so that exhaustive 
feature engineering is possible for MT.

\section{Visualization}

We have introduced tools for visualizing two of the main data structures used
in Joshua \cite{PBML-2010-Josua-visualization}. Each visualization tool is implemented as a
stand-alone program that hooks into the pipeline, but they are distributed with
Joshua. The first program is a hypergraph visualizer. The user can choose from
a set of input sentences, then call the decoder to build the hypergraph.

The second is a derivation-tree visualizer. Joshua can produce output formatted
as a parse tree, where each nonterminal has been annotated with its source-side
span. The visualizer can read in multiple n-best lists in this format, then
display the resulting derivation trees side-by-side. We have found that
visually inspecting these derivation trees is advantageous in debugging many
grammars.

We would like to add visualization tools for more parts of the pipeline. For
example, a chart visualizer would make it easier for researchers to tell where
search errors were happening during decoding, and why. An alignment visualizer
for aligned parallel corpora might help to determine how grammar extraction 
could be improved.


\section{MT Pipeline}
\todo{lane}

\bibliographystyle{acl}
\bibliography{machinetranslation}

\end{document}
