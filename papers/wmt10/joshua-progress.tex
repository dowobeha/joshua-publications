%
% File acl2010.tex
%
% Contact  jshin@csie.ncnu.edu.tw or pkoehn@inf.ed.ac.uk
%%
%% Based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\newcommand{\joshua}{\textbf{Joshua}}

\title{Joshua 2.0: An Open Source Toolkit for Machine Translation \\with Syntax, Semirings and Other Goodies }

\author{
Zhifei Li,\,\,\,
Chris Callison-Burch,\,\,\,
Chris Dyer,$^\dagger$\,\,\,
Juri Ganitkevitch,\,\,\,
\\ {\bf
Ann Irvine,\,\,\,  
Sanjeev Khudanpur,\,\,\,
Lane Schwartz,$^\star$\,\,\, 
Wren N.\,G.\,Thornton,\,\,\,}
\\ {\bf
Ziyuan Wang,\,\,
Jonathan Weese\,\,
{\textnormal{and}}
\,\,\,Omar F. Zaidan
}\\
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD\\
$\dagger$ Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD\\
$\star$ Natural Language Processing Lab, University of Minnesota, Minneapolis, MN }


\date{}

\begin{document}


\maketitle
\begin{abstract}
We describe the progress we have made in the past year to \textbf{Joshua} \cite{joshua-wmt09}, 
an open-source toolkit for parsing-based machine translation.
The main new functions we have added into the toolkit include: supporting for the syntax-augmented machine translation (SAMT) grammar,
supporting mannual constraints in the input to constrain the decoding,  
semiring parsing for unified dynamic programming, hypergraph-based 
discriminative training for better feature engineering, visualization of the derivation
trees and hypergraphs, and a unified pipeline for MT experiments.
\end{abstract}


\listoftodos

\section{Introduction}

\textbf{Joshua} \cite{joshua-wmt09} is an open-source toolkit (written in Java) for parsing-based machine translation.
The first version implemented all the essential algorithms described 
in \newcite{hiero-2007}: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and $k$-best extraction.  
The initial release also implemented suffix-array grammar extraction \cite{lopez:2007:EMNLP-CoNLL2007} 
and minimum error rate training \cite{och-mert}. 
Additionally, parallel and distributed computing techniques were exploited 
to make it scalable \cite{Joshua-old}. In the design of \joshua, we have applied general
principles of software engineering to achieve three
major goals: extensibility, end-to-end coherence,
and scalability.

The toolkit has then been used by the authors and several
other groups in their daily research, and thus much progress has
been made to the first version. 
In this report, we will describe the main new functions we have added into the 
toolkit, which include: supporting for the syntax-augmented machine translation (SAMT) grammar \cite{???},
supporting mannual constraints in the input to constrain the decoding \cite{???},  
semiring parsing for unified dynamic programming \cite{li-eisner:2009:EMNLP}, 
hypergraph-based discriminative training for better feature engineering \cite{oracle-extraction-naacl09}, 
visualization of the derivation trees and hypergraphs \cite{???}, 
and a unified pipeline for MT experiments \cite{???}.

Below we give a short description for each of the new functions added.


\section{Support for Syntax Augmented MT}
\todo{juri or ccb}


\section{Manual Constraints on the Input}
\todo{anni}

\section{Lattice Parsing}
\todo{Chris Dyer}


\section{Semiring Parsing}

In \joshua, we use a {\em hypergraph} (or {\em packed forest}) to compactly 
represent the exponentially many derivation trees generated by the decoder for a 
particular foreign input sentence.
Given a hypergraph, we may perform many atomic inference operations
such as finding one-best, $k$-best, or expectations over the hypergraph.
To perform each operation, we may implement a dedicated dynamic programming algorithm.
However, a more general framework to specify these algorithms is semiring-weighted parsing \cite{semiringparsing}.
Within this framework, we implemented all the algorithms (i.e.,
inside, outside, and inside-outside speedup) described by \newcite{li-eisner:2009:EMNLP},
and also the first-order expectation semiring \cite{eisner-expectation-semiring} and its
second-order version \cite{li-eisner:2009:EMNLP}.

% The original first-order expectation semiring allows us to efficiently
% compute a vector of first-order statistics (expectations; first
% derivatives) on the set of paths in a lattice or the set of trees in a
% hypergraph.  The second-order expectation semiring {\em additionally} computes
% a matrix of second-order statistics (expectations of {\em products}; second
% derivatives (Hessian); derivatives of expectations).

%% use of expectation and variance semiring
The first- and second-order expectation semirings can be used compute many interesting quantities over the hypergraph.
These quantities include expected hypothesis length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence,
Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on.
% The second-order expectation semiring is essential for many interesting training paradigms 
% such as deterministic annealing \cite{Rose98deterministicannealing}, minimum risk \cite{smith-eisner:2006:POS},
% active and semi-supervised learning \cite{entropy-mini,semiCRF-jiao}.
% In these settings, we must compute the gradient of entropy or risk. 
% The semiring can also be used for second-order gradient optimization algorithms.



\section{Hypergraph-based Discriminative Training}
\todo{zhifei}

\cite{variational-decoding-acl09}
\cite{oracle-extraction-naacl09}

\section{Visualization}

We have introduced tools for visualizing two of the main data structures used
in Joshua \cite{joshua-viz}. Each visualization tool is implemented as a
stand-alone program that hooks into the pipeline, but they are distributed with
Joshua. The first program is a hypergraph visualizer. The user can choose from
a set of input sentences, then call the decoder to build the hypergraph.

The second is a derivation-tree visualizer. Joshua can produce output formatted
as a parse tree, where each nonterminal has been annotated with its source-side
span. The visualizer can read in multiple n-best lists in this format, then
display the resulting derivation trees side-by-side. We have found that
visually inspecting these derivation trees is advantageous in debugging many
grammars.

We would like to add visualization tools for more parts of the pipeline. For
example, a chart visualizer would make it easier for researchers to tell where
search errors were happening during decoding, and why. An alignment visualizer
for aligned parallel corpora might help to determine how grammar extraction 
could be improved.


\section{MT Pipeline}
\todo{lane}

\bibliographystyle{acl}
\bibliography{machinetranslation}

\end{document}
