%
% File acl2010.tex
%
% Contact  jshin@csie.ncnu.edu.tw or pkoehn@inf.ed.ac.uk
%%
%% Based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xspace}

%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\newcommand{\joshua}{\textbf{Joshua}\xspace}
\newcommand{\Q}{\mathcal{Q}}

\title{Joshua 2.0: A Toolkit for Parsing-Based Machine Translation \\with Syntax, Semirings, Discriminative Training and Other Goodies }

\author{
Zhifei Li,\,\,\,
Chris Callison-Burch,\,\,\,
Chris Dyer,$^\dagger$\,\,\,
Juri Ganitkevitch,\,\,\,
\\ {\bf
Ann Irvine,\,\,\,  
Sanjeev Khudanpur,\,\,\,
Lane Schwartz,$^\star$\,\,\, 
Wren N.\,G.\,Thornton,\,\,\,}
\\ {\bf
Ziyuan Wang,\,\,
Jonathan Weese\,\,
{\textnormal{and}}
\,\,\,Omar F. Zaidan
}\\
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD\\
$\dagger$ Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD\\
$\star$ Natural Language Processing Lab, University of Minnesota, Minneapolis, MN }


\date{}

\begin{document}


\maketitle
\begin{abstract}
We describe the progress we have made in the past year on \textbf{Joshua} \cite{joshua-wmt09}, 
an open source toolkit for parsing based machine translation.
The new functionality includes: 
support for translation grammars with a rich set of syntactic nonterminals,
the ability for external modules to posit constraints on spans in the input sentence should be translated,  
lattice parsing for dealing with input uncertainty,
a semiring framework that provides a unified way of doing various dynamic programming calculations, 
variational decoding for approximating the intractable MAP decoding,
hypergraph-based discriminative training for better feature engineering, 
visualization of the derivation trees, 
and a cleaner pipeline for MT experiments.
\end{abstract}



\section{Introduction}

\textbf{Joshua} is an open-source toolkit for parsing-based machine translation that is written in Java. The initial release of Joshua  \cite{joshua-wmt09} was a re-implementation of the Hiero system \cite{hiero-2007} and all its associated algorithms, including: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and $k$-best extraction.  The Joshua 1.0 release also included re-implementations of suffix-array grammar extraction \cite{lopez:2007:EMNLP-CoNLL2007,PBML-2010-Joshua-grammar-extraction}  and minimum error rate training \cite{och-mert,Zaidan2009}.  Additionally, it included parallel and distributed computing techniques for salability \cite{Joshua-old}. 

This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release.  The most important new functions in the toolkit are: 
\begin{itemize}
\item
Support for any style of synchronous context free grammar (SCFG)
% instead of just Hiero grammars.  This 
including syntax augment machine translation (SAMT) grammars \cite{samt2006},
\item
Support for external modules to posit translations for spans in the input sentence that constrain decoding,    \cite{PBML-2010-Joshua-transliteration},  
\item
Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters  \cite{dyer-muresan-resnik:2008:ACLMain},
\item
Semiring parsing for unified dynamic programming \cite{li-eisner:2009:EMNLP} , 
\item
Improvements to decoding through variational methods and approximations to intractable MAP decoding \cite{variational-decoding-acl09},
\item
Hypergraph-based discriminative training for better feature engineering \cite{zhifei-forest-reranking-galebook}, 
\item
Visualization of the derivation trees and hypergraphs \cite{PBML-2010-Joshua-visualization}, 
\item
A convenient framework for designing and running reproducible machine translation experiments \cite{Schwartz-wmt10-pipline}.
\end{itemize}
Below we give a short description for each of the new functions added.


\section{Support for Syntax Augmented MT}
\todo{juri or ccb}


\section{Specifying Constraints on Rules}
\todo{anni}

\section{Semiring Parsing}

In \joshua, we use a {\em hypergraph} (or {\em packed forest}) to compactly 
represent the exponentially many derivation trees generated by the decoder for a 
particular foreign input sentence.
Given a hypergraph, we may perform many atomic inference operations
such as finding one-best, $k$-best, or expectations over the hypergraph.
To perform each operation, we may implement a dedicated dynamic programming algorithm.
However, a more general framework to specify these algorithms is semiring-weighted parsing \cite{semiringparsing}.
Within this framework, we implemented all the algorithms (i.e.,
inside, outside, and inside-outside speedup) described by \newcite{li-eisner:2009:EMNLP},
and also the first-order expectation semiring \cite{eisner-expectation-semiring} and its
second-order version \cite{li-eisner:2009:EMNLP}.

% The original first-order expectation semiring allows us to efficiently
% compute a vector of first-order statistics (expectations; first
% derivatives) on the set of paths in a lattice or the set of trees in a
% hypergraph.  The second-order expectation semiring {\em additionally} computes
% a matrix of second-order statistics (expectations of {\em products}; second
% derivatives (Hessian); derivatives of expectations).

%% use of expectation and variance semiring
The first- and second-order expectation semirings can be used compute many interesting quantities over the hypergraph.
These quantities include expected hypothesis length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence,
Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on.

% The second-order expectation semiring is essential for many interesting training paradigms 
% such as deterministic annealing \cite{Rose98deterministicannealing}, minimum risk \cite{smith-eisner:2006:POS},
% active and semi-supervised learning \cite{entropy-mini,semiCRF-jiao}.
% In these settings, we must compute the gradient of entropy or risk. 
% The semiring can also be used for second-order gradient optimization algorithms.

\section{Lattice Parsing}

The bottom-up parsing algorithm used to generate the translation hypergraph has been generalized to support translation from not only an input sentence but from a \emph{word lattice} with runtime and memory overhead that is proportional to the size of the lattice, rather than the number of paths in the lattice \cite{dyer-muresan-resnik:2008:ACLMain}.  This enables the exploration of a distribution over input sentences during decoding, and the best translation is selected from among any of them.  This functionality is useful when \joshua is used to translate the output of statistical preprocessing components, such as speech recognizers or word segmenters.

\section{Variational Decoding}

Statistical models in machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
during decoding is then NP-hard.
The first version of \joshua implemented the
Viterbi approximation that measures the goodness 
of a string using only its most probable derivation.

The Viterbi approximation is efficient, but it ignores most of the derivations in the hypergraph.
We implemented variational decoding \cite{variational-decoding-acl09}, which works as follows.
First, given a foreign string (or lattice), the MT system produces a hypergraph, 
which encodes a probability distribution $p$ over 
possible output strings and their derivations.
Second, a distribution $q$ is selected that approximates $p$ as well as possible but comes from a family of distributions $\mathcal{Q}$ in which inference is tractable.  Third, the best string according to $q$ (instead of $p$) is found.
In our implementation, the $q$ distribution is parameterized by an $n$-gram model, under which 
the second and third steps can be performed efficiently and exactly via dynamic programming.
In this way, variational decoding considers all derivations in the hypergraph but still
allows tractable decoding. 



\section{Hypergraph-based Discriminative Training}

Discriminative training with a large number of features has 
potential to improve the MT performance.
We have implemented the hypergraph-based minimum risk training \cite{li-eisner:2009:EMNLP},
which minimizes the {\em expected loss} of the reference translations.
The minimum-risk objective can be optimized by a gradient-based method, where
the risk and its gradient can be computed using a second-order expectation semiring.
For optimization, we use both L-BFGS\footnote{http://en.wikipedia.org/wiki/L-BFGS} 
and Rprop\footnote{http://en.wikipedia.org/wiki/Rprop}.

We have also implemented the average Perceptron algorithm and forest-reranking \cite{zhifei-forest-reranking-galebook}.
Since the reference translation may not be in the hypergraph due to pruning or inherent
defficiency of the translation grammar, we need to use an {\em oracle translation} (i.e., the translation in
the hypergraph that is most simmilar to the reference translation) as a surrogate for training.
We implemented the {\em oracle extraction} algorithm described by \newcite{oracle-extraction-naacl09}
for this purpose.

Given the current infrastructure, other training methods 
(e.g., maximum conditional likelihood or MIRA as used by \newcite{chiang-knight-wang:2009:NAACLHLT09})
can also be easily supported with minimum coding.
We plan to implement a large number of feature functions in \joshua so that exhaustive 
feature engineering is possible for MT.

\section{Visualization}

We have introduced tools for visualizing two of the main data structures used
in Joshua \cite{PBML-2010-Joshua-visualization}. Each visualization tool is implemented as a
stand-alone program that hooks into the pipeline, but they are distributed with
Joshua. The first program is a hypergraph visualizer. The user can choose from
a set of input sentences, then call the decoder to build the hypergraph.

The second is a derivation-tree visualizer. Joshua can produce output formatted
as a parse tree, where each nonterminal has been annotated with its source-side
span. The visualizer can read in multiple n-best lists in this format, then
display the resulting derivation trees side-by-side. We have found that
visually inspecting these derivation trees is advantageous in debugging many
grammars.

We would like to add visualization tools for more parts of the pipeline. For
example, a chart visualizer would make it easier for researchers to tell where
search errors were happening during decoding, and why. An alignment visualizer
for aligned parallel corpora might help to determine how grammar extraction 
could be improved.


\section{MT Pipeline}

Reproducing other researchers' machine translation experiments is difficult because the pipeline is too complex to fully detail in short conference papers. We have put together a workflow framework for designing and running reproducible machine translation experiments using Joshua \newcite{Schwartz-wmt10-pipline}. Each step in the machine translation workflow (data preprocessing, grammar training, MERT, decoding, etc) is modeled by a Make script that defines how to run the tools used in that step, and an auxiliary configuration file that defines the exact parameters to be used in that step for a particular experimental setup. Workflows configured using this framework allow a complete experiment to be run - from downloading data and software through scoring the final translated results - by executing a single Makefile.

This framework encourages researchers to supplement research publications with links to the complete set of scripts and configurations that were actually used to run the experiment. The Johns Hopkins University submission to the WMT10 shared translation task was implemented in this framework, and can be easily and exactly reproduced by other researchers by typing a single command. 
\bibliographystyle{acl}
\bibliography{machinetranslation}

\end{document}
