\documentclass[logo]{pbml}

\usepackage{times}
\usepackage{float}
\usepackage{multirow}
\usepackage{amscd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{CJK}


\newcommand{\newcite}{\cite}

\begin{document}

\title{Decoding in Joshua}
\subtitle{Open Source, Parsing-Based Machine Translation}


\author{firstname=Zhifei, surname=Li,
       address={Center for Language and Speech Processing, Johns Hopkins University, Baltimore MD 21218, USA}}
\author{firstname=Chris, surname=Callison-Burch, address={Center for Language and Speech Processing, Johns Hopkins University, Baltimore MD 21218, USA}}
\author{firstname=Sanjeev, surname=Khudanpur, address={Center for Language and Speech Processing, Johns Hopkins University, Baltimore MD 21218, USA}}
\author{firstname=Wren, surname=Thornton, address={Center for Language and Speech Processing, Johns Wren University, Baltimore MD 21218, USA}}

\shorttitle{Decoding in Joshua}
\shortauthor{Z. Li, C. Callison-Burch, S. Khudanpur, W. Thornton}



\maketitle



\begin{abstract}
We describe a scalable decoder for parsing-based machine translation. The decoder is written in Java and implements all the essential algorithms described in \newcite{Chiang2007} and \newcite{Joshua-old}: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and $k$-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in Python.

\end{abstract}


%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Motivation}
%%tree-based translation
Large-scale parsing-based statistical machine translation has made significant progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax.
For instance, the hierarchical translation system of \newcite{Chiang2007} extracts a synchronous grammar from pairs of strings, whereas \newcite{Quirk2005}, \newcite{Liu2006} and \newcite{Huang2006b} perform syntactic analyses in the source-language, and \newcite{Galley2006} uses target-language syntax.


%%complexity of a parsing-based decoder, and our decoder
A critical component in parsing-based MT systems is the decoder, which is complex to implement and scale up for large data sets. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in \cite{Huang2005,Chiang2007,Huang2007}, it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems. In this paper, we describe an important first-step towards an extensible, general-purpose, scalable, and open-source parsing-based MT decoder. Our decoder is written in Java and implements all the essential algorithms described in \newcite{Chiang2007}: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and unique $k$-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable.

%%experiments, and open source release
We demonstrate experimentally that our decoder is 38 times faster than a previous decoder written in Python. Furthermore, the distributed computing permits improving translation quality via large-scale language models. The decoder has been used to translate roughly a million sentences in a parallel corpus for large-scale discriminative training experiments \cite{Li2008}. The decoder has also been successfully used by other researchers. For example, \newcite{Chen2008} have demonstrated that our decoder achieves performance competitive with Moses \cite{Moses}, another major open-source machine translation toolkit.
We hope the release of the decoder will greatly contribute the progress of the syntax-based machine translation research.


%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Parsing-based MT Decoder}\label{sec-parsing}
In this section, we discuss the core algorithms implemented in our decoder. These algorithms have been discussed by \newcite{Chiang2007} in detail, and we recapitulate the essential parts here for completeness.\footnote{Most of the descriptions here are adopted from \newcite{Joshua-old}.}


\subsection{Grammar Formalism}

\begin{CJK}{GB}{gbsn}

Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Following the notation in \newcite{Venugopal2007}, a probabilistic SCFG comprises a set of source-language terminal symbols, $T_{S}$, a set of target-language terminal symbols, $T_{T}$, a shared set of nonterminal symbols, $N$, and a set of rules of the form
\begin{equation}
%\vspace{-0.07in}
 \label{eqn-grammar-rule}
\text{X} \rightarrow \langle \gamma, \alpha, \sim, w \rangle
\end{equation}
where $X\in N$, ~$\gamma \in [N \cup T_{S}]^\ast$ is a (mixed) sequence of nonterminals and source terminals, $\alpha \in [N \cup T_{T}]^\ast$ is a sequence of nonterminals and target terminals, $\sim$ is a one-to-one correspondence or \emph{alignment} between the nonterminal elements of $\gamma$ and $\alpha$, and $w\geq0$ is a weight assigned to the rule. An illustrative rule for Chinese-to-English translation is
\[
\text{NP} \rightarrow \langle\, \text{NP}_{0} \textit{~µÄ~} \text{NP}_{1}\,,\,\text{NP}_{1} \textit{~of~} \text{NP}_{0}\,\rangle
\]
where the Chinese word µÄ (pronounced \textit{de} or \textit{di}) means \textit{of}, and the alignment, encoded via subscripts on the nonterminals, causes the two noun phrases around µÄ to be reordered around \textit{of} in the translation. The rule weight is omitted in this example.

\end{CJK}


A bilingual SCFG derivation is analogous to a monolingual CFG derivation. It begins with a pair of \emph{aligned} start symbols. At each step, an \emph{aligned} pair of nonterminals is rewritten as the two corresponding components of a single rule. In this sense, the derivations are generated synchronously.

Our decoder presently handles SCFGs of the kind extracted by Heiro \cite{Chiang2007}, but is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars \cite{Eisner2003,Chiang2006}.

\subsection{MT Decoding as Chart Parsing}\label{subsec-parsing}
Given a source-language sentence, $f^\ast$, the decoder must find the target-language yield, $e(D)$, of the derivation $D$ which has the best composite weight, $w(D)$, among all derivations whose source-language yield, $f(D)$, is the source-language sentence. Or equationally,
\begin{equation}
%\vspace{-0.07in}
 \label{eqn-bad-update}
e^\ast \,\,\,=\,\,\, e\!\left(\operatornamewithlimits{arg\,max}_{D\,:\,f(D) = f^\ast} w(D)\right)
\end{equation}
The composite weight is a linear combination of feature function weights and feature function values. General feature functions include translation model features, language model features, and word penalty features.

The actual decoding algorithm  maintains a \emph{chart}, which contains an array of \emph{cells}. Each cell in turn maintains a list of proven \emph{items}. The parsing process starts with the axioms, and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The new item also maintains backpointers to antecedent items, which are used for $k$-best extraction, as discussed in Section \ref{subsec-kbest} below.

In a SCFG-based decoder, an \emph{item} is identified by its source-language span, left-side nonterminal label, and left- and right-contexts for the target-language $n$-gram LM. Therefore, in a given cell, the maximum possible number of items is $O(|N|\,|T_{T}|^{2(n-1)})$, and the worst case decoding complexity is
\begin{equation}
\label{eqn:decoding-complexity}
O\!\left(l^3\,|N|^K\,|T_{T}|^{2K(n-1)}\right)
\end{equation}
where $K$ is the maximum number of nonterminal pairs per rule and $l$ is the source-language sentence length \cite{Venugopal2007}.


\subsection{Pruning in a Decoder}\label{subsec-prune}
Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies and detailed nonterminal sets. In our decoder, we incorporate two pruning techniques described by \cite{Chiang2007,Huang2007}. For \emph{beam pruning}, in each cell, we discard all items whose weight is $\beta$-times worse than the weight of the best item in the same cell. If too many items pass that relative threshold, then only the top $b$ items by weight are retained in each cell. When applying an inference rule to combine smaller items and obtain a larger item, we use \emph{cube pruning} to simulate $k$-best extraction in each destination cell, discarding combinations which lead to an item whose weight is worse than the best item in that cell by a margin of $\epsilon$.

\subsection{Hyper-graphs and $k$-best Extraction}\label{subsec-kbest}
For each source-language sentence the output of the chart-parsing algorithm may be treated as a \emph{hyper-graph} representing a set of likely derivation hypotheses. Briefly, a hyper-graph is a set of \emph{vertices} and \emph{hyper-edge}s, with each hyper-edge connecting a \emph{set} of antecedent vertices to a consequent vertex, and a special vertex designated as the \emph{target vertex}. In parsing parlance, a vertex corresponds to an item in the chart, a hyper-edge corresponds to a SCFG rule with the nonterminals on the right-side replaced by back-pointers to antecedent items, and the target vertex corresponds to the goal item\footnote{In a decoder integrating an $n$-gram LM, there may be multiple goal items due to different LM contexts. However, one can image a \emph{single} goal item identified by the span $[0,n]$ and the goal nonterminal $S$, but not by the LM contexts.}.

Given a hyper-graph for a source-language sentence $f^\ast$, we use the $k$-best extraction algorithm of \newcite{Huang2005} to extract its $k$ most likely translations. Moreover, since many different derivations may lead to the same target-language yield $e(D)$, we adopt the modification described in \newcite{Huang2006b} to efficiently generate the \emph{unique} $k$ best translations of $f^\ast$.


%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Underlying Methodologies}
When designing our decoder we applied principles of software engineering to improve usability and hence utility to open-source users. Our three major design goals are: extendibility, end-to-end coherence, and scalability.


%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Extendibility}
To make Joshua a suitable baseline for future research it is necessary that it be easily extended by other researchers. As befitting a project of its size, the Joshua code is organized into separate \emph{packages} for each major aspect of functionality (e.g.\,chart parsing, feature functions, and hyper-graph algorithms). In this way it is clear which files contribute to a given functionality and researchers can focus on a single package without worrying about the rest of the system.

Illicit interactions and unseen dependencies are a common hinderance to extensibility in large projects. To minimize these problems, all extensible components are defined by Java \emph{interfaces}. The interfaces are designed to be minimalistic so that they do not hinder radical departures from current implementations, such as using per-sentence or non-trie-based translation grammars. Where there is a clear point of departure for research, a basic implementation of each interface is provided as an \emph{abstract class} to minimize the work necessary for new extensions.

A non-exhaustive list of future extensions we envisioned when designing our interfaces include:
\begin{itemize}
\item
	Using a new decoding algorithm such as agenda-based parsing, instead of the default CKY algorithm;
\item
	Adding new pruning algorithms, beside the already implemented beam- and cube-pruning;
\item
	Using grammars with linguistic syntax such as the grammar described in \newcite{Galley2006,Venugopal2009}, rather than Hiero-style grammars;
\item
	Handling non-SCFG grammar formalisms, e.g.\,synchronous tree substitution grammars \cite{Eisner2003};
\item
	Adding new feature functions, e.g.\,the source-side syntax constraints described by \newcite{Marton2008};
\item
	Using novel language models like the bloom-filter LM described in \newcite{Talbot2007a}, not just ARPA backoff $n$-gram models;
\item
	Adding new algorithms that operate on the hyper-graph, for example, hyper-graph reranking or discriminative training over the hyper-graph.
\end{itemize}


%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{End-To-End Cohesion}
There are many components to a machine translation pipeline aside from the decoder. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hindering repeatability of experiments.

To combat these issues, the Joshua toolkit integrates other critical components of the machine translation pipeline as well as the decoder. Two critical components being integrated are suffix-array grammar extraction \cite{Callison-Burch2005b,Lopez2007} and minimum error rate training (MERT) \cite{Och2003c,Bertoldi2009,Zaidan2009}. Additional components we hope to integrate include tools for building language models and generating word alignments, as well as a general infrastructure for configuring and connecting segments of the pipeline.

For researchers who have already invested much work into their pipelines, the decoder can be treated as a stand-alone tool and does not rely on the rest of the toolkit we provide.


%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Scalability}
Our third design goal was to ensure that the decoder is scalable to large models and data sets. The parsing and pruning algorithms are carefully implemented with dynamic programming strategies, and efficient data structures are used to minimize overhead.

The integration of suffix-array grammar extraction and MERT also contributes to scalability. Suffix arrays are compact data structures which can store many more $n$-grams than a traditional phrase table with the same memory footprint. They are also amenable to extracting small per-sentence grammars on the fly, rather than needing a monolithic grammar for the entire test set. With MERT integration we do not need to start a new decoder instance each iteration, which means we can load the grammar into memory once (an expensive task compared to the decoding time itself) instead of repeatedly.

We also implement \emph{parallel decoding} and a \emph{distributed language model}. Parallel decoding is able to exploit multi-core and multi-processor architectures by translating multiple sentences in separate threads and storing the language model and translation grammar in shared memory. Enabling the distributed language model reduces memory pressure and makes it feasible to use large LMs by running the LM on a separate machine from the decoder or decoders. More details on these two features are provided in \cite{Joshua-old}.



%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Using the Decoder}\label{sec-instructions}
To produce a translation output for a test document, one needs to follow the following general five-step procedure.

\begin{enumerate}
\item
	Train a language model using a toolkit such as the SRI LM tools \cite{Stolcke2002};
\item
	Extract a translation grammar for the test set. This step itself involves several sub-steps, e.g.\,preparing a bilingual corpus, obtaining word alignments with a tool like GIZA \cite{Och2003}, and extracting the grammar using the suffix-array infrastructure;
\item
	Find optimal weights for combining the different models and feature functions by using MERT or another training procedure;
\item
	Write the decoder's configuration file, specifying the language model, translation model, feature weights, and other options. The integrated MERT, when given an initial configuration file, will produce a modified configuration with the final weights. Table \ref{tbl-config} shows an example configuration file.
\begin{table}[h]
\begin{center}
\begin{tabular}{ |l|}\hline

\# lm file location\\
lm\_file$=$example.trigram.lm.gz\\
\\
\# tm file location\\
tm\_file$=$example.hiero.tm.gz\\
\\
\# lm model weight\\
lm 1.000000\\
\\
\# translation model weights\\
phrasemodel pt 0 1.066893\\
phrasemodel pt 1 0.752247\\
phrasemodel pt 2 0.589793\\
\\
\# wordpenalty weight\\
wordpenalty -2.844814\\ \hline
\end{tabular}
\end{center}
\caption{An example configuration file. For conciseness, this file neglects some standard configuration options (e.g.\,$k$-best size).}
\label{tbl-config}
\end{table}

\item
	Finally, run the decoder to produce the $k$ best translations for each sentence in the test document. For an input file, \texttt{test.in}, an output $k$-best file, \texttt{test.kbest}, and a configuration file, \texttt{config}, the decoder can be invoked with:\\\\ \texttt{java joshua.JoshuaDecoder config test.in test.kbest}\\\\Often it is helpful to pass additional flags to the JVM to specify the minimum and maximum size of the heap, to adjust the minimum free-heap ratio, or to enable 64-bit mode.
% The verbatim character \char`\\ is prettier here than \textbackslash
\end{enumerate}



%% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Experimental Results}
In this section, we evaluate the performance of our decoder on a large-scale Chinese to English translation task.\footnote{Again, most of the descriptions here are adopted from \newcite{Joshua-old}.}

\subsection{System Training}
We use various parallel text corpora distributed by the Linguistic Data Consortium (LDC) for the NIST MT evaluation. The parallel data we select contains about 570K Chinese-English sentence pairs, adding up to about 19M words on each side. To train the English language models, we use the English side of the parallel text and a subset of the English Gigaword  corpus, for a total of about 130M words.

We use the GIZA toolkit \cite{Och2003}, a suffix-array architecture \cite{Lopez2007}, the SRILM toolkit \cite{Stolcke2002}, and minimum error rate training \cite{Och2003c} to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively.


\subsection{Improvements in Decoding Speed}
%%%lzf add umd here
We use a Python implementation of a state-of-the-art decoder as our baseline\footnote{We are extremely thankful to Philip Resnik at University of Maryland for allowing us the use of their Python decoder as the baseline. Thanks also go to David Chiang who originally implemented the decoder.} for decoder comparisons. For a direct comparison, we use exactly the same models and pruning parameters. The SCFG contains about 3M rules, the $5$-gram LM explicitly lists about 49M $n$-grams, $n=1, 2, \ldots, 5$, and the pruning uses $\beta=10$, $b=30$ and $\epsilon=0.1$.
\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|c|c|}\hline
\multirow{2}{*}{ Decoder}&	Speed&	\multicolumn{2}{c|}{ BLEU-4}\\\cline{3-4}
&					(sec/sent)&	MT '03&	MT '05\\\hline\hline
Python&			26.5&			34.4\%&		32.7\%\\\hline\hline
Java&				1.2&			\multirow{2}{*}{{\bf \textcolor{blue}{34.5\%}}}&
									\multirow{2}{*}{{\bf \textcolor{blue}{32.9\%}}}\\\cline{1-2}
Java (parallel)&	\bf \textcolor{blue}{0.7}&		&\\\hline
%excluding loading time for tm/lm (Java is also faster than Python in this part)
%lm:5-gram, 49M lines; TM: 3.4M rules
\end{tabular}
\end{center}
\caption{Decoder Comparison: Translation speed and quality on the 2003 and 2005 NIST MT benchmark tests.}
\label{results-speed}
\end{table}

As shown in Table \ref{results-speed}, the Java decoder (without explicit parallelization) is 22 times \emph{faster} than the Python decoder, while achieving slightly \emph{better} translation quality as measured by BLEU-4 \cite{Papineni2002}. The parallelization further speeds it up by a factor of 1.7, making the parallel Java decoder is 38 times faster than the Python decoder.

We have also used the decoder to successfully decode about one million sentences for a large-scale discriminative training experiment \cite{Li2008}, showing that the decoder is stable and scalable.

\subsection{Impact of a Distributed Language Model }
We use the SRILM toolkit to build eight $7$-gram language models, and load and call the LMs using a distributed LM architecture as discussed before. As shown in Table \ref{results-dlm}, the $7$-gram distributed language model (DLM) significantly improves translation performance over the $5$-gram LM.
However, decoding is significantly slower (12.2 sec/sent when using the non-parallel decoder) due to the added network communication overhead.
\begin{table}[h]
\begin{center}
\begin{tabular}{l c c c c}\hline
LM type&		\# $n$-grams&	MT '03&	MT '05 \\ \hline
$5$-gram  LM&	49\,M&	34.5\%&		32.9\%\\
$7$-gram DLM&   310\,M&	{\bf \textcolor{blue}{35.5\%}}&	{\bf \textcolor{blue}{33.9\%}}\\\hline
\end{tabular}
\end{center}
\caption{Distributed language model: the $7$-gram LM cannot be loaded alongside the SCFG on a single machine; via distributed computing, it yields significant improvement in BLEU-4 over a $5$-gram.}
\label{results-dlm}
\end{table}

%NIST MT08 experiments
%System                                                                        MT03(DEV)                            MT05
%UMD Decoder+UMD-5-gram-LM+nonum                         34.38                                     32.71
%UMD Decoder+UMD-5-gram-LM+withnum                       ?                                           33.13
%Java Decoder+UMD-5-gram-LM+nonum                          34.53                                     32.85
%Java Decoder+JHU-7-gram-LM+nonum                           35.49                                     33.88
%Java Decoder+JHU-7-gram-LM+beam50+nonum              35.69                                    34.26
%Java Decoder+JHU-7-gram-LM+beam150+nonum            35.92                                     ?(failed due to out of memory)
%
%UMD Final submission                                                  35.07                                     33.71


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-.5cm}
\section{Conclusions}
We have described a scalable decoder for parsing-based machine translation. It is written in Java and implements all the essential algorithms described in \newcite{Chiang2007} and \newcite{Joshua-old}: chart-parsing, $n$-gram language model integration, beam- and cube-pruning, and unique $k$-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We demonstrate that our decoder is 38 times faster than a baseline decoder written in Python, and that the distributed language model is very useful to improve translation quality in a large-scale task.
The decoder has been used for decoding millions of sentences for a large-scale discriminative training task \cite{Joshua-old}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgments}
We would like to thank the other Joshua developers for their contributions to the code: Chris Dyer, Lane Schwartz, Jonathan Weese, and Omar Zaidan. We also thank Adam Lopez, Smaranda Muresan and Philip Resnik  for very helpful discussions. This research was supported in part by the Defense Advanced Research Projects Agency's GALE program under Contract No.\,HR0011-06-2-0001 and the National Science Foundation under grants Numbers 0713448 and 0840112. The views and findings are the authors' alone.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{../bibliography}
\end{document} 