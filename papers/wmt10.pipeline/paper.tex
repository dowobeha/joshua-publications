\documentclass[11pt, a4paper]{article}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{tikz}
\usetikzlibrary{fit,shapes.misc,shapes}
\usepackage{acl2010}
%\usepackage[pdftex]{graphicx}
%\usepackage[colorinlistoftodos]{todonotes}


\title{Reproducible Results in Parsing-Based Machine Translation:\\
The JHU Shared Task Submission}

\author{Lane Schwartz\
\thanks{\ Research conducted as a visiting researcher at Johns Hopkins University}\
\\University of Minnesota\\Minneapolis, MN\\
{\tt lane@cs.umn.edu}% \And
%Chris Callison-Burch \\
%Johns Hopkins University \\
%Baltimore, MD\\
%{\tt ccb@cs.jhu.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present the Johns Hopkins University submission to the 2010 WMT shared translation task. We describe processing steps using open data and open source software used in our submission, and provide the scripts and configurations required to train, tune, and test our machine translation system.
\end{abstract}


\section{Introduction}

Research investigating natural language processing and computational linguistics can and should have an extremely low barrier to entry. The data with which we work is customarily available in common electronic formats. The computational techniques which we apply can typically be performed on commodity computing resources which are widely available. In short, there should be no reason why small research groups and even lone researchers should not be able to join and make substantive contributions furthering our field.
%
The reality is less encouraging. 

Many published articles describe novel techniques and provide interesting results, yet fail to describe technical details in sufficient detail to allow their results to be reproduced by other researchers. While there are notable and laudable exceptions, many publications fail to provide the source code and scripts necessary to reproduce results. The use of restricted data, not freely available for download by any interested researcher only compounds these problems. \newcite{pedersen2010} rightly argues that the implementation details so often ignored in publications are in fact essential for our research to be reproducible science.

Reproducibility in machine translation is made more challenging by the complexity of experimental workflows. Results in machine translation tasks are dependent on a cascade of processing steps and configurations. While interesting subsets of these usually appear in experimental descriptions, many steps (preprocessing techniques, alignment parameters, translation rule extraction parameters, language model parameters, list of features used) are invariably omitted, even though these configurations are often critical to reproducing results.

This paper describes the Johns Hopkins University submission to the 2010 Workshop on Statistical Machine Translation shared translation task. Links to the software, scripts, and configurations used to run the experiments described herein are provided. The remainder of this paper is structured as follows. Section \ref{Related Work} lists the major examples of publicly available open source machine translation systems, parallel corpora, and machine translation workflow management systems. Section \ref{Framework} describes the experimental workflow used to run the shared task translations, with the corresponding experimental design in section \ref{Design}. Section \ref{Results} presents the shared task results.






\begin{figure*}[htbp]
\include{figure.pgf}
\caption{Machine translation workflow. Square nodes in grey indicate software and scripts. The scripts and configuration files used to implement and run this workflow are available for download at \url{http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-experiment.tgz}}
\label{dependencyGraph}
\end{figure*}


\section{Related Work}
\label{Related Work}

The last four years have witnessed the implementation and release of numerous open source machine translation systems. The widely used Moses system \cite{moses} implements the standard phrase-based translation model. Parsing-based translation models are implemented by Joshua \cite{Joshua-WMT}, SAMT \cite{samt2006}, and cdec \cite{cdec}. Cunei \cite{Phillips2009} implements statistical example-based translation. \newcite{Phramer} and \newcite{schwartz08mclc} respectively provide additional open-source implementations of phrase-based and hierarchical decoders.

The SRILM \cite{Stolcke2002}, IRSTLM \cite{IRSTLM}, and RandLM \cite{Talbot2007a} toolkits enable efficient training and querying of n-gram language models. 

Freely available parallel corpora for numerous European languages have also been released in recent years. These include the Europarl \cite{Koehn-europarl} and JRC-Acquis \cite{Steinberger-2006} legislative corpora, each of which includes data for most EU language pairs. The smaller News Commentary corpora \cite{Callison-Burch2007a,Callison-Burch2008a} provide smaller amounts of parallel data in the news genre. The recent Fr-En $10^9$ \cite{WMT09-Findings} corpus aggregates huge numbers of parallel French-English sentences from the web.

Open source systems to address the complex workflows required to run non-trivial machine translation experiments have also been developed. These include {\tt experiment.perl} \cite{experiment.perl}, developed as a workflow management system at the University of Edinburgh, and LoonyBin \cite{LoonyBin}, a general hyperworkflow management utility from Carnegie Melon University.







\section{Experimental Workflow Management}
\label{Framework}


Running a statistical machine translation system to achieve state-of-the-art performance involves the configuration and execution of numerous interdependent intermediate tools. To manage task dependencies and tool configuration, our shared task workflow consists of a set of dependency scripts written for GNU Make \cite{gnumake}. 

Figure \ref{dependencyGraph} shows a graph depicting the steps in our experimental workflow, and the dependencies between steps. Each node in the graph represents a step in the workflow; each step is implemented as a Make script that defines how to run the tools required in that step. In each experiment, an additional configuration script is provided for each experimental step, defining the parameters to be used when running that step in the current experiment. Optional front-end wrapper scripts can also be provided, allowing for a complete experiment to be run - from downloading data and software through truecasing translated results - by executing a single make file.

This framework is also conducive to parallelization. Many tasks, such as preprocessing numerous training files, are not dependent on one another. In such cases {\tt make} can be configured to execute multiple processes simultaneously on a single multi-processor machine. In cases where scheduled distributed computing environments such as the Sun Grid Engine are configured, make files can be processed by scheduler-aware {\tt make} variants ({\tt distmake}, SGE {\tt qmake}, Sun Studio {\tt dmake}) which distribute outstanding tasks to available distributed machines using the relevant distributed scheduler.





\section{Experimental Configuration}
\label{Design}

\begin{table*}[htb]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\bf Source & \bf Target & \bf Parallel Corpora \\ \hline
German & English & news-commentary10.de-en europarl-v5.de-en \\ \hline
English & German & news-commentary10.de-en europarl-v5.de-en \\ \hline
French & English & news-commentary10.fr-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr \\ \hline
English & French & news-commentary10.fr-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr \\ \hline
Spanish & English & news-commentary10.es-en europarl-v5.es-en undoc.2000.en-es \\ \hline
English & Spanish & news-commentary10.es-en europarl-v5.es-en undoc.2000.en-es  \\ \hline
\end{tabular}
\end{center}
\caption{Parallel training data used for training translation model, per language pair}
\label{parallelCorpora}
\end{table*}

\begin{table*}[htb]
\begin{center}
\begin{tabular}{|l|l|}
\hline
\bf Target & \bf Monolingual Corpora \\ \hline
English & europarl-v5.en news-commentary10.en news.en.shuffled undoc.2000.en-fr.en giga-fren.release2.en \\ \hline
French & europarl-v5.fr news-commentary10.fr news.fr.shuffled undoc.2000.en-fr.fr giga-fren.release2.fr \\ \hline
German & europarl-v5.de news-commentary10.de news.de.shuffled \\ \hline
Spanish & europarl-v5.es news-commentary10.es news.es.shuffled undoc.2000.en-es.es\\ \hline
\end{tabular}
\end{center}
\caption{Monolingual training data used for training language model, per target language}
\label{monolingualCorpora}
\end{table*}


Experimental workflows were configured\footnote{http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-experiment.tgz} and run for six language pairs in the translation shared task: English-French, English-German, English-Spanish, French-English, German-English, and Spanish-English. 

In all experiments, only data freely available for download was used. No restricted data from the LDC or other sources was used. Table \ref{parallelCorpora} lists the parallel corpora used in training the translation model for each experiment. The monolingual corpora used in training each target language model are listed in table \ref{monolingualCorpora}. In all experiments, news-test2008 was used as a development tuning corpus during minimum error rate training; newstest2009 was used as a development test set. The shared task data set newstest2010 was used as a final blind test set.

All data was automatically downloaded, unzipped, and preprocessed prior to use. Files provided in XML format were converted to plain text by selecting lines with {\tt <seg>} tags, then removing the beginning and end tags for each segment; this processing was applied using GNU grep and sed. The {\tt tokenize.perl} and {\tt lowercase.perl} scripts provided for the shared task\footnote{http://www.statmt.org/wmt08/scripts.tgz with md5sum: \\ {\tt tokenize.perl} 45cd1832827131013245eca76481441a \\ {\tt lowercase.perl} a1958ab429b1e29d379063c3b9cd7062} were applied to all data.

Interpolated n-gram language models for the four target languages were built using the SRI Language Model Toolkit\footnote{ http://www-speech.sri.com/projects/srilm \\ SRILM version 1.5.7. Our experimental workflow requires that SRILM be compiled separately, with the \$SRILM environment variable set to the install location.}, with n-gram order set to 5. The \newcite{ChenGoodman98} technique for modified Kneser-Ney discounting \cite{Kneser1995} was applied during language model training.

Following \newcite{Joshua-WMT}, a subset of the available training sentences was selected via subsampling; training sentences are selected based on the estimated likelihood of each sentence being useful later for translating a particular test corpus. 


Given a subsampled parallel training corpus, word alignment is performed using the Berkeley aligner\footnote{http://berkeleyaligner.googlecode.com/files/berkeleyaligner\\\_unsupervised-2.1.tar.gz --- Berkeley aligner version 2.1} \cite{liang06alignment}. 

For each language pair, a synchronous context free translation grammar is extracted for a particular test set, following the methods of \newcite{Lopez2008} as implemented in \cite{Schwartz-PBML}. For the largest training sets (French-English and English-French) the original \cite{Lopez2008} implementation included with Hiero was used to save time during training\footnote{It is expected that using the Joshua implementation should result in nearly identical results, albeit with somewhat more time required to extract the grammar.}.

Because of the use of subsampling, the extracted translation grammars are targeted for use with a specific test set. Our experiments were begun prior to the release of the blind newstest2010 shared task test set. Subsampling was performed for the development tuning set, news-test2008, and the development test set, newstest2009. Once the newstest2010 test set was released, the process of subsampling, alignment, and grammar extraction was repeated to obtain translation grammars targeted for use with the shared task test set. 

Our experiments used hierarchical phrase-based grammars containing exactly two nonterminals - the wildcard nonterminal X, and S, used to glue together neighboring constituents. Recent work has shown that parsing-based machine translation using SAMT \cite{samt2006} grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs \cite{SCALE-report}. Joshua supports such grammars; the experimental workflow presented here could easily be extended in future research to incorporate the use of SAMT grammars with additional language pairs. 

The Z-MERT implementation \cite{Zaidan2009} of minimum error rate training \cite{Och2003c} was used for parameter tuning\footnote{Z-MERT seed value was set to 12341234}. Tuned grammars were used by Joshua to translate all test sets. The Joshua decoder produces n-best lists of translations. 

Rather than simply selecting the top candidate from each list, we take the preferred candidate after perform minimum Bayes risk rescoring \cite{Kumar2004b}.

Once a single translation has been extracted for each sentence in the test set, we repeat the procedures described above to train language and translation models for use in translating lowercased results into a more human-readable truecased form. A truecase language model is trained as above, but on the tokenized (but not normalized) monolingual target language corpus. Monotone word alignments are deterministically created, mapping normalized lowercase training text to the original truecase text. As in bilingual translation, subsampling is performed for the training set, and a translation grammar for lowercase-to-truecase is extracted. No tuning is performed. The Joshua decoder is used to translate the lowercased target language test results into truecase format. The {\tt detokenize.perl} and {\tt wrap-xml.perl} scripts provided for the shared task were manually applied to truecased translation results prior to final submission of results.


The code used for subsampling, grammar extraction, decoding, minimum error rate training, and minimum Bayes risk rescoring is provided with Joshua\footnote{http://sourceforge.net/projects/joshua/files/joshua/1.3/joshua-1.3.tgz/download --- Joshua version 1.3}, with the exception of the original \cite{Lopez2008} grammar extraction implementation.










\section{Experimental Results}
\label{Results}

The experiments described in sections \ref{Framework} and \ref{Design} above provided truecased translations for six language pairs in the translation shared task: English-French, English-German, English-Spanish, French-English, German-English, and Spanish-English. Table \ref{scores} lists the automatic metric scores for the newstest2010 test set, according to the BLEU \cite{Papineni2002} and TER \cite{TER} metrics. 

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l||c|c|c|}
\hline
\bf Source & \bf Target & \bf BLEU & \bf BLEU- & \bf TER \\
& & & \bf cased & \\
\hline
German & English & 21.3 & 19.5 & 0.660 \\ \hline
English & German & 15.2  & 14.6  & 0.738 \\ \hline
French & English & 27.7 & 26.4 & 0.614 \\ \hline
English & French & 23.8 & 22.8 & 0.681 \\ \hline
Spanish & English & 29.0 & 27.6 & 0.595 \\ \hline
English & Spanish & 28.1 & 26.5 & 0.596  \\ \hline
\end{tabular}
\end{center}
\caption{\label{scores} Automatic metric scores for the test set newstest2010 }
\end{table}

The submitted system ranked highest among shared task participants for the German-English task, according to TER.

In order to provide points of comparison with the 2009 Workshop on Statistical Machine Translation shared translation task participants,  table \ref{devtest-scores} lists automatic metric scores for our systems' translations of the newstest2009 test set, which we used as a development test set.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l||c|}
\hline
\bf Source & \bf Target & \bf BLEU  \\
\hline
German & English & 18.19  \\ \hline
English & German & 13.57 \\ \hline
French & English & 26.41 \\ \hline
English & French & 25.28 \\ \hline
Spanish & English & 25.28 \\ \hline
English & Spanish & 24.02  \\ \hline
\end{tabular}
\end{center}
\caption{\label{devtest-scores} Automatic metric scores for the development test set newstest2009}
\end{table}





\section*{Acknowledgements}
This work was supported by the DARPA GALE program (Contract No HR0011-06-2-0001).





\bibliographystyle{acl}
\bibliography{bibliography}

\end{document}
