\documentclass[11pt]{article}
%\usepackage[%
%  colorlinks,%
%  linkcolor=black,%
%  anchorcolor=black,%
%  citecolor=black,%
%  filecolor=black,%
%  menucolor=black,%
%  pagecolor=black,%
%  urlcolor=black,%
%  %hyperindex=false,%
%  bookmarksopen=true,%
%  pdfusetitle,%
%  pdfpagelabels,
%  naturalnames
%]{hyperref}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\usepackage[colorlinks]{hyperref}

\usepackage{tikz}
\usetikzlibrary{fit,shapes.misc,shapes}

\usepackage{acl2010}
\usepackage[colorinlistoftodos]{todonotes}


%\title{Empirical Questions in Machine Translation\
 %--- Reproducible Results for the JHU WMT10 System}
\title{Reproducible Results in Parsing-Based Machine Translation:\\
The JHU Shared Task Submission}

\author{Lane Schwartz\
\thanks{\ Research conducted as a visiting researcher at Johns Hopkins University}\
\\University of Minnesota\\Minneapolis, MN\\
{\tt lane@cs.umn.edu} \And
Chris Callison-Burch \\
Johns Hopkins University \\
Baltimore, MD\\
{\tt ccb@cs.jhu.edu}}

\date{}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

%\listoftodos

\section{Introduction}

%\todo[inline,color=yellow,caption={Call to action}]{

Research investigating natural language process and computational linguistics can and should have an extremely low barrier to entry. The data with which we work is customarily available in common electronic formats. The computational techniques which we apply can typically be performed on commodity computing resources which are widely available. In short, there should be no reason why small research groups and even lone researchers should not be able to join and make substantive contributions furthering our field.
%
The reality is less encouraging. 

Many published articles describe novel techniques and provide interesting results, yet fail to describe technical details in sufficient detail to allow their results to be reproduced by other researchers. While there are notable and laudable exceptions, many publications fail to provide the source code and scripts necessary to reproduce results. The use of private data, not freely available for download by any interested researcher only compounds these problems. \newcite{pedersen2010} rightly argues that the implementation details so often ignored in publications are in fact essential for our research to be reproducible science.

Reproducibility in machine translation is made more challenging by the complexity of experimental workflows. Results in machine translation tasks are dependent on a cascade of processing steps and configurations. While interesting subsets of these usually appear in experimental descriptions, many steps (preprocessing techniques, alignment parameters, translation rule extraction parameters, language model parameters, list of features used) are invariably omitted, even though these configurations are often critical to reproducing results.

This paper describes the Johns Hopkins University submission to the 2010 Workshop on Statistical Machine Translation shared translation task. Links to the software, scripts, and configurations used to run the experiments described herein are provided. The remainder of this paper is structured as follows. Section \ref{Related Work} lists the major examples of publicly available open source machine translation systems, parallel corpora, and machine translation workflow management systems. Section \ref{Framework} describes the experimental design used to run the shared task translations. Section \ref{Results} presents the shared task results.




\section{Related Work}
\label{Related Work}

The last four years have witnessed the implementation and release of numerous open source machine translation systems. The widely used Moses system implements the standard phrase-based translation model. Parsing-based translation models are implemented by SAMT \cite{samt2006} and Joshua \cite{Joshua-WMT}. Cunei \cite{Phillips2009} implements statistical example-based translation. The SRILM \cite{Stolcke2002} and IRSTLM \cite{IRSTLM} toolkits enable efficient training and querying of n-gram language models. \newcite{Phramer} and \newcite{schwartz08mclc} provide other open-source implementations of phrase-based and hierarchical decoders, respectively.

Freely available parallel corpora for numerous European languages have also been released in recent years. These include the Europarl \cite{Koehn-europarl} and JRC-Acquis \cite{Steinberger-2006} legislative corpora, each of which includes data for most EU language pairs. The smaller News Commentary \cite{Callison-Burch2007a,Callison-Burch2008a} provide smaller amounts of parallel data in the news genre. The recent Fr-En $10^9$ \cite{WMT09-Findings} corpus aggregates huge numbers of parallel French-English sentences from the web.

Systems to address the complex workflows required to run non-trivial machine translation experiments have also been developed. These include {\tt experiment.perl} \cite{experiment.perl}, developed as a workflow management system at the University of Edinburgh, and LoonyBin \cite{LoonyBin}, a general hyperworkflow management utility from Carnegie Melon University.

\todo[inline,caption={Parsing-based SMT}] {
Hiero grammar extraction \cite{Lopez2008}
}

\section{Framework}
\label{Framework}

%\missingfigure{Graph of dependencies and steps}
\begin{figure*}
%\include{mt-workflow-preprocess}
\include{figure.pgf}
%\begin{tikzpicture}[shape=rounded rectangle]
%\node (PreprocessLabel) {\underline{\bf Preprocess Data}};
%\node (CompressedData) [style=draw,below of= PreprocessLabel]  {Compressed Data};
%\node (DecompressedData) [style=draw,below of=CompressedData] {Decompressed Data};a
%\node (RemoveXML) [style=draw,below of=DecompressedData,text width=28mm]  {Test Set Data Stripped of XML};
%\node (Tokenize) [style=draw,below of= RemoveXML]  {Tokenized Data};
%\node (Normalize) [style=draw,below of=Tokenize] {Normalized Data};
%\node [style=draw,shape=rectangle,fit=(PreprocessLabel) (CompressedData) (DecompressedData) (RemoveXML) (Tokenize) (Normalize)] {};
%
%\draw [->] (CompressedData) -- (DecompressedData);
%\draw [->] (DecompressedData) -- (RemoveXML);
%\draw [->] (RemoveXML) -- (Tokenize);
%\draw [->] (Tokenize) -- (Normalize);
%\draw [->] (DecompressedData) -- (Tokenize);
%\end{tikzpicture}

\caption{Machine translation workflow. The scripts and configuration files used to implement and run this workflow are available for download at \url{http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-experiment.tgz}}
\end{figure*}

GNU Make \cite{gnumake}

\todo[inline,color=yellow,caption={parallel make}]{
Many tasks, such as preprocessing numerous training files, are not dependent on one another. In such cases {\tt make} can be configured to execute multiple processes simultaneously on a single multi-processor machine. In cases where scheduled distributed computing environments such as the Sun Grid Engine are configured, make files can be processed by scheduler-aware {\tt make} variants ({\tt distmake}, SGE {\tt qmake}, Sun Studio {\tt dmake}) which distribute outstanding tasks to available distributed machines using the relevant distributed scheduler.
}

\todo[inline,caption={experimental design}] {

\begin{itemize}
\item Data used: Europarl \cite{Koehn-europarl}, Fr-En $10^9$ \cite{WMT09-Findings}, News Commentary \cite{Callison-Burch2007a,Callison-Burch2008a}. No restricted data from the LDC. 
\item Preprocessing: Removed XML markup; Tokenized and lowercased training data using scripts provided for the WMT10 shared task
\item Training corpus subsampling, as described in \cite{Joshua-WMT}
\item Corpus aligned using Berkeley aligner \cite{liang06alignment}
\item Grammar extraction using Joshua suffix array grammar extraction \cite{Schwartz-PBML}; for Fr-En corpus used \cite{Lopez2008} for speed
\item Language models built using SRILM \cite{Stolcke2002}.
\item Decoder used was Joshua \cite{Joshua-WMT}
\item Minimum error rate training \cite{Zaidan2009}
\end{itemize}
}

\todo[inline,color=yellow] {
Our experiments used hierarchical phrase-based grammars containing exactly two nonterminals - the wildcard nonterminal X, and S, used to glue together neighboring constituents. Recent work has shown that parsing-based machine translation using SAMT \cite{samt2006} grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs \cite{SCALE-report}. Joshua supports such grammars; the experimental workflow presented here could easily be extended in future research to incorporate the use of SAMT grammars with additional language pairs.
}


\section{Reproducible Results}
\label{Results}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l||c|c|c|}
\hline
\bf Source & \bf Target & \bf BLEU & \bf BLEU- & \bf TER \\
& & & \bf cased & \\
\hline
German & English & 21.3 & 19.5 & 0.660 \\ \hline
English & German & 15.2  & 14.6  & 0.738 \\ \hline
French & English & 27.7 & 26.4 & 0.614 \\ \hline
English & French & 23.8 & 22.8 & 0.681 \\ \hline
Spanish & English & 29.0 & 27.6 & 0.595 \\ \hline
English & Spanish & 28.1 & 26.5 & 0.596  \\ \hline
\end{tabular}
\end{center}
\caption{\label{scores} Automatic metric scores for the test set newstest2010 }
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l||c|}
\hline
\bf Source & \bf Target & \bf BLEU  \\
\hline
German & English & 18.19  \\ \hline
English & German & 13.57 \\ \hline
French & English & 26.41 \\ \hline
English & French & 25.28 \\ \hline
Spanish & English & 25.28 \\ \hline
English & Spanish & 24.02  \\ \hline
\end{tabular}
\end{center}
\caption{\label{devtest-scores} Automatic metric scores for the development test set newstest2009}
\end{table}

\todo[inline,caption={URL}]{
List how to download: \url{http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-experiment.tgz}
}

\section*{Acknowledgements}
This work was supported by the DARPA GALE program (Contract No HR0011-06-2-0001).

\bibliographystyle{acl}
\bibliography{bibliography}

\end{document}
